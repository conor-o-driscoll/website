[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Conor O'Driscoll",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nERSA 2025: Some Reflections\n\n\n\n\n\nMy thoughts and reflections on the 65th annual conference of the European Regional Science Association.\n\n\n\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nPopulations and Samples\n\n\n\n\n\nDiscussing the difference between statistical populations and samples\n\n\n\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nDatasets and Data Types As Seen in R\n\n\n\n\n\nA practical application of some of my earlier posts.\n\n\n\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe Three P’s: Programming, Packages, and Projects\n\n\n\n\n\nDigging a bit deeper into RStudio\n\n\n\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Started With R and RStudio\n\n\n\n\n\nNavigating the installation process of R and RStudio while also introducing you to the RStudio environment.\n\n\n\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nDifferent Types of Data: Part 2\n\n\n\n\n\nQualitative or Quantitative? Nominal, Ordinal, Interval, and Ratio.\n\n\n\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nDifferent Types of Data: Part 1\n\n\n\n\n\nInternal and External. Primary and Secondary.\n\n\n\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Data?\n\n\n\n\n\nData describe the universe we wish to study.\n\n\n\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Is Statistics?\n\n\n\n\n\nstatistics is how we learn from evidence.\n\n\n\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nThinking Statistically: A Primer\n\n\n\n\n\nThinking statistically doesn’t start with numbers or formulas, it starts with noticing.\n\n\n\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\nHow to write a thesis in 1000 words or less\n\n\n\n\n\nAre you struggling to write your thesis?\n\n\n\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nAlways visualise your data: Simpsons Paradox in action\n\n\n\n\n\nWhat if the data you’re analyzing tells one story in aggregate—but the exact opposite when you break it down?\n\n\n\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nStart Young, Drive Less: The Overlooked Power of the School-Run\n\n\n\n\n\nWhat if the key to transforming urban mobility isn’t the daily commute—but how we get our kids to school?\n\n\n\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the cost of commuting?\n\n\n\n\n\nHave you ever wondered how much your daily commute costs you? This post may provide you with some answers.\n\n\n\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to my website!\n\n\n\n\n\nWelcome to my website! This post describes what you can expect to find on this platform and how I intend to use it.\n\n\n\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "Curriculum Vitae: Download pdf here",
    "section": "",
    "text": "Personal Profile\nAssistant Professor in The University of Groningen’s Department of Economic Geography. Main fields of interest include Economic Geography, Regional Science, Urban Economics, and Transportation Science. Proficient in spatial analysis which employs advanced econometric techniques (STATA, QGIS, and R).\n\n\nEducation\n\nPhD: Economics\nDepartment of Economics, University College Cork, Ireland\nOctober 2020 – September 2023\n\nDissertation Title: Urban Sprawl: Land-Use, Travel Behaviours, and Emissions in Ireland\nSupervisors: Prof. Justin Doran, Dr. Frank Crowley, Dr. Nóirín McCarthy\nPhD Committee: Prof. Aisling Reynolds-Feighan (UCD), Dr. John Eakins (UCC), Dr. Edward Lahiff (UCC)\n\n\n\nBA: Economics and History\nUniversity College Cork, Ireland\nSeptember 2017 – September 2020\n\nGrade: Upper Second-Class Honours (3.33–3.67 GPA)\nMinor Dissertation: First-Class Honours (4.0 GPA)\n\n\n\n\nEmployment History\n\nAssistant Professor in Economic Geography\nFaculty of Spatial Sciences, University of Groningen, The Netherlands\nOctober 2023 – Present\n\n\nSenior Demonstrator in Economics\nCentre for Policy Studies, University College Cork, Ireland\nSeptember 2022 – June 2023\n\n\nTeaching Assistant in Economics\nDepartment of Economics, University College Cork, Ireland October 2020 – June 2023\n\n\nPostgraduate Tutor in Business, Social Sciences, and Public Policy\nSkills Centre, University College Cork, Ireland\nJune 2021 – June 2023\n\n\nResearch Assistant\nDepartment of Economics, University College Cork, Ireland April 2021 – July 2022; July 2023 – September 2023\n\n\n\nTeaching Experience\n\nEconomic Geography\n\n2024–: Economic Geography: Theory and Practice (MSc)\n2023–: Economic Geography (BSc)\n\n\n\nStatistics / Econometrics / Quantitative Research Methods\n\n2023–: Introductory Statistics (BSc); Intermediate Statistics (BSc); Quantitative Research Methods (BSc).\n2020-2023: Data Collection, Analysis, and Interpretation (BA); Quantitative Research Methods (BA); Mathematics For Business (BComm); Research in Economics (BA).\n\n\n\nEconomics\n\n2020-2023: Introductory Microeconomics (BSc, BComm, BA); Intermediate Microeconomics (BComm, BA); Introductory Macroeconomics (BSc, BComm, BA); Intermediate Macroeconomics (BComm, BA); Economics and Social Issues (BComm, BA); Economics and Labour Markets (BComm); Money and Monetary Policy (BComm).\n\n\n\nResearch Supervision\n\nPhD Proposals [2]; MSc Dissertations [14]; Undergraduate Dissertations/Projects [25].\n\n\n\n\nAccolades and Honours\n\n2023: Postgraduate Researcher of The Year (2021-2022)\nCollege of Business and Law, University College Cork, Ireland\n\n\n2022: Best Paper by an Early Career Researcher - Honourable Mention\nRegional Science Association International: British and Irish Section\n\n\n\nGrants and Funding Awards\n\n2025:\nUniversity of Groningen PhD Starter Grant co-funded by Talent in de Regio (€300,000 + €50,000) With Dr. Femke Cnossen and Prof. Sierdjan Koster\n\n\n2023:\nCork University Business School Travel Bursary (€800) University College Cork, Department of Economics Conference Bursary (€400)\n\n\n2022:\nUCC Department of Economics Conference Bursary (€400)\n\n\n2021:\nUCC Department of Economics Conference Bursary (€450)\n\n\n\nAffiliations and Professional Service\n\nResearch Affiliations\n\nResearch Affiliate\nRudolf Agricola School for Sustainable Development, University of Groningen January 2025 – Present\n\n\nResearch Associate\nThe Urban and Regional Studies Institute, University of Groningen October 2023 – Present\n\n\nResearch Associate\nSpatial and Regional Economic Research Centre, University College Cork October 2020 – Present\n\n\n\nEditorial Positions\n\nChief Editor of The Boolean\nJune 2021 – September 2023\n\n\n\nReferee Services\nJournal of Transport and Land-Use, Transportation Research Interdisciplinary Perspectives, Journal of Geographical Systems, Urban Climate, The Boolean, The Spatial and Regional Economic Research Centre PhD Development Series.\n\n\nProfessional Memberships\n\nWorld Society of Transport and Land Use Research\n2025 – Present\n\n\nRegional Studies Association\n2023 – Present\n\n\nRegional Science Association International\n2020 – Present\n\n\n\n\nPublications\n\nOngoing Research Projects\n\nConor O’Driscoll. 2025. “Commuting in Flux: The Roles of Place and Personal Circumstance In Shaping Behavioural Plasticity”.\nConor O’Driscoll and Milad Abbasiharofteh. 2025. “Roots and Routes: Residential Relocation and Relatedness”.\nConor O’Driscoll and Luise Koeppen. 2025. “Subnational Institutional Corruption and Political Discontent”.\nConor O’Driscoll and Federica Rossi. 2025. “Residential Relocation Decisions and Destinations: The Role of Working From Home”. Under Review in Regional Studies.\nConor O’Driscoll and Ana Maria Silva. 2025. “Working From Home and Labour Market Outcomes: The Case of Earnings and Hours Worked”. Under Review in Oxford Economic Papers.\nConor O’Driscoll. 2025. “Navigating Change: Residential Relocation, Travel Behaviours, and Built Environments”. Major Revisions in Papers in Regional Science.\n\n\n\nPeer-Reviewed Journal Articles\n\nConor O’Driscoll; Frank Crowley; Justin Doran; Nóirín McCarthy; Josh O’Driscoll. 2025. “Travel Behaviours and Built Environments on School-Runs”. Regional Science Policy and Practice, 17(1), pp.1-14. doi: 10.1016/j.rspp.2024.100153.\nKevin Credit and Conor O’Driscoll. 2024. “Assessing Modal Tradeoffs and Associated Built Environment Characteristics Using a Cost-Distance Framework”. Journal of Transport Geography, 117, pp.1-19. doi: 10.1016/j.jtrangeo.2024.103870.\nConor O’Driscoll; Frank Crowley; Justin Doran; Nóirín McCarthy. 2024. “The relationships between socio-demographics, residential environments, travel considerations, and commute mode choice in Ireland”. Regional Studies, 58(3), pp.1-18. doi: 10.1080/00343404.2023.2199779.\nConor O’Driscoll; Frank Crowley; Justin Doran; Nóirín McCarthy. 2023. “Land-Use Mix in Ireland: Implications for Sustainable Development”. Journal of Maps, 19(1), pp.1-7. doi: 10.1080/17445647.2023.2214165.\nConor O’Driscoll; Frank Crowley; Justin Doran; Nóirín McCarthy. 2023. “Land-Use Mixing in Irish Cities: Implications for Sustainable Development”. Land Use Policy, 128(5), pp.1-7. doi: 10.1016/j.landusepol.2023.106615.\nConor O’Driscoll; Frank Crowley; Justin Doran; Nóirín McCarthy. 2022. “Retail Sprawl and CO2 Emissions: Retail Centres in Irish Cities”. Journal of Transport Geography, 102(6), pp.1-12. doi: 10.1016/j.jtrangeo.2022.103376.\n\n\n\nIndustry and Policy Reports\n\nJane Bourke; Conor O’Driscoll; Josh O’Driscoll; Páidí O’Reilly. 2022. “Innovating in Ireland: Can We Fail Better?”. Cork University Business School.\n\n\n\nSelect Popular Writings\n\nConor O’Driscoll. 2024. “The Cost of Morning Commutes: The Case of Kildare”. Kildare FM.\nConor O’Driscoll and Kevin Credit. 2024. “Here’s the real cost of your morning rush hour commute in Dublin”. RTE Brainstorm.\nConor O’Driscoll; Frank Crowley; Justin Doran; Nóirín McCarthy. 2023. “How the relationship between socio- demographics, residential environments and travel influence commuter choices”. Regional Studies Blog.\nConor O’Driscoll. 2023. “Retail Centre Locations in Cork: The Case of Carrigtwohill”. RTE Prime Time.\nConor O’Driscoll; Frank Crowley; Justin Doran; Nóirín McCarthy. 2022. “Planning For Sustainability: Future Retail Centre Locations”. The Boolean, 6(8), pp.27-32.\nConor O’Driscoll; Frank Crowley; Justin Doran; Nóirín McCarthy. 2022. “The Links Between Where We Live and How We Commute”. RTE Brainstorm.\nJane Bourke; Josh O’Driscoll; Conor O’Driscoll. 2022. “Does fear of failure hamper Irish business innovation?”. RTE Brainstorm.\nConor O’Driscoll. 2021. “Why out of town retail parks don’t make sense in a climate crisis”. RTE Brainstorm.\nConor O’Driscoll. 2021. “Shopping Malls, GHG Emissions and The Role of Policymakers in”Green” Transportation Infrastructure in Ireland”. Regional Studies Association: Student Summer Series.\n\n\n\n\nMain Conferences and Workshops\n\n2025:\nRegional Science Association International: British and Irish Section; the 64th European Regional Science Association (ERSA) Congress (Scheduled).\n\n\n2024:\nRegional Studies Association: Irish Section; Regional Science Association International: British and Irish Section.\n\n\n2023:\nRegional Science Association International: British and Irish Section; Cork University Business School Postgraduate Symposium.\n\n\n2022:\nRegional Science Association International: British and Irish Section (Session Chair); Cork University Business School Postgraduate Symposium.\n\n\n2021:\nConference of Irish Geographers; Regional Science Association International: British and Irish Section (Organising Committee Member); Cork University Business School Postgraduate Symposium; UCC Spatial and Economic Research Centre Lunchtime Speaker Series; The North American Meetings of The Regional Science Association (Session Chair).\n\n\n\nReferences\n\nProf. Justin Doran: Primary PhD Supervisor and Head of UCC’s Department of Economics. Email: Justin.Doran@ucc.ie.\nProf. Sierdjan Koster: Dean of RUG’s Faculty of Spatial Sciences. Email: sierdjan.koster@rug.nl.\nProf. Frank Crowley: Secondary PhD Supervisor. Email: Frank.Crowley@ucc.ie\nDr. Viktor Venhorst: Associate Professor in RUG’s Department of Economic Geography. Email: v.a.venhorst@rug.nl."
  },
  {
    "objectID": "posts/data.html",
    "href": "posts/data.html",
    "title": "What is Data?",
    "section": "",
    "text": "Statistics and Data: A Refresher\nSometimes, we observe a single thing — a person, a place, an event — and note several of its features. More interestingly, we often observe many things that are similar in some respects but quite different in others. We notice patterns across people, places, or time. We end up with a collection of observations, or, in the language of statistics, data.\n\n\nWhat Is Data?\nAt their simplest, data are pieces of information — observations about the world that we record in some form. A single measurement, like the temperature outside your window at 9 a.m., the price of a coffee, or the number of emails you received yesterday, is a data point. These individual observations can seem small or insignificant on their own, but they become powerful when we begin to organize, compare, and analyze them.\nData are the raw material on which the discipline of statistics is built, as well as the raw material from which individual statistics are calculated. They are how we turn experience into insight. Just as a painter starts with brushstrokes and a sculptor with clay, researchers, policymakers, and businesses start with data. Every chart, table, and model in the world begins with some observation of something, somewhere, at some time.\nWhen most people think of data, they think of numbers — prices, temperatures, test scores, population counts. Numbers are easy to compare, summarize, and analyze, which is why quantitative data dominate most datasets.\nBut data don’t have to be numerical. Words, categories, images, and even sounds can be data if they represent something about the world in a structured way. For example, a survey might record people’s favorite color, a collection of photos could capture land use patterns, or a transcript of an interview could be coded into themes for analysis.\nThe key is that data are representations. They simplify some aspect of reality so we can work with it. Whether numbers or categories, quantitative or qualitative, the purpose is the same: to turn observations into something we can organize, compare, and interpret.\n\n\nFrom Data Points To Datasets\nA single observation rarely tells us much. To understand patterns, we need more than one data point — we need a collection of observations, or a dataset. Datasets allow us to ask questions like: How do house prices vary across neighborhoods? How does rainfall change over the course of a year? How do exam scores differ between schools?\nDatasets give structure to our observations. They let us see variation, compare groups, and detect trends. A few data points can suggest a pattern, but a well-structured dataset allows us to test whether that pattern is real, typical, or just a fluke. It’s the difference between noticing one expensive apartment in a city and understanding the broader reality of housing affordability, for example.\n\n\nMeasurement and Interpretation\nData are not just facts handed down by nature. They are created, measured, and recorded by humans (or human-designed instruments). This introduces choices and interpretations at every stage: What should we measure? How should we measure it? When and where should we record it? Even seemingly simple decisions — like whether to record height in centimeters or inches — can shape the analysis that follows.\nBecause of this, data always carry context. Understanding that context is part of thinking statistically. A number without context can mislead; it’s only by knowing how, why, and under what conditions it was collected that we can use it wisely and interpret it correctly. For example, if you are using survey data, it is important to think about whether everyone asked gave answers to all the questions. If not, is there some systematic bias which might explain why some people answered and others did not? Similarly, we need to know if the data we are using are up to date and whether the instrument use to measure and collect data is reliable.\n\n\nData Quality and Representation\nOne way to look at data is to view it as evidence. Without data, our ideas and theories about the world are little more than speculations. Thus, data provide a grounding, linking our ideas to reality and allowing us to validate and test our understanding.\nNot all data are created equal. Some are precise and reliable; others are messy, inconsistent, or incomplete. Poor-quality data can lead to misleading conclusions, no matter how sophisticated the analysis. Data quality depends on factors such as accuracy, completeness, consistency, and timeliness.\nFor example, consider a dataset of city rental prices. If a few entries are wrong — a typo that lists €10,000 instead of €1,000 — the average rent can be skewed, giving a false impression of affordability. Missing data, such as unreported rents in certain neighborhoods, can bias conclusions. Even the way categories are defined, like “downtown” or “suburban,” can affect what the data seem to show.\nRecognizing the limitations of data is just as important as analyzing the numbers themselves. Good data analysis begins with careful inspection: understanding what is measured, checking for errors, and considering what might be missing.\nThe choices we make in measurement matter. For example, if we measure student performance only through test scores, we ignore other important aspects like creativity, teamwork, or critical thinking. Similarly, using surveys to measure happiness depends on how questions are phrased and who responds. Measurement is never perfect, but careful design can make data useful and meaningful.\nData can also summarize, categorize, and structure information. A dataset may record exact values (e.g., temperature), counts (e.g., number of hospital visits), or categories (e.g., occupation type). The way data are structured influences what we can ask and what conclusions we can draw.\n\n\nWrapping Up\nUltimately, data are the bridge between the world we see and the patterns we try to understand. Individual data points become datasets; datasets reveal variation, relationships, and trends; and careful attention to quality and measurement ensures that the patterns we see are meaningful and trustworthy.\nThinking critically about data — where it comes from, how it was collected, and what it actually represents — is as important as any statistical technique. Raw numbers alone do not explain anything. It is through analysis, comparison, and interpretation that data become insight, guiding decisions, shaping policies, and helping us make sense of complex social, economic, and natural phenomena.\n\n\nBibliography\n\nStatistics: A Very Short Introduction, by David J. Hand"
  },
  {
    "objectID": "posts/diff_data_2.html",
    "href": "posts/diff_data_2.html",
    "title": "Different Types of Data: Part 2",
    "section": "",
    "text": "Different Types of Data Collection\nWhen we talk about “data,” we’re really talking about many different things at once: how the information was collected, how it is structured, and how it can be measured. Understanding these distinctions is the first step toward using data responsibly and effectively.\nData can be collected in many different ways. Surveys and questionnaires ask people directly about their opinions, behaviors, or experiences. Experiments create controlled situations designed to test specific hypotheses. Observations involve recording what you see in the world without interfering. Administrative records capture information as part of routine operations, like hospital admissions or school enrollments. Increasingly, digital traces—such as website clicks or social media activity—provide vast new sources of information.\nEach of these methods has strengths and limitations: surveys capture personal perspectives but depend on honest responses, administrative data is often large and reliable but may miss key details, and experiments are powerful for identifying cause and effect but can be costly or artificial. The way data is collected shapes what it can and cannot tell you.\n\n\nThe Structure of a Dataset\nBroadly speaking, it is convenient to regard data as having two aspects: one aspect covers the objects we wish to study (e.g., schoolchildren), the other covers the characteristics of those objects (e.g., test scores). In statistics, it is common to call these characteristics variables, with each object having a value for every variable under study.\nIn any one study, we might be interested in multiple kinds of objects. we might want to understand and make statements not only about schoolchildren, but also about the schools they attend, the neighbourhoods they live in, the quality of teachers within their school etc. Moreover, we will typically not be interested in any single variable, but rather the relationships between different variables. We may even be interested in seeing how these relationships differ across different types of objects (e.g., boys and girls).\nOnce collected, data is usually organized into a dataset. At its simplest, a dataset is a table in which the rows represent cases (also called observations or units) and the columns represent variables. Cases might be individuals, households, companies, cities, or even single transactions. Variables describe characteristics of those cases, such as age, income, location, or test scores. For example, in a dataset on students, each row might represent a student, while the columns record their gender, age, hours studied, and exam results. Thinking in terms of cases and variables is fundamental because it frames how we analyze data.\n\nNow seems like a good moment to stop and test yourself to ensure that you truly understand what you are reading. Indicate whether each of the statements below is TRUE or FALSE.\n\nIn a typical dataset, columns represent different characteristics of the objects being studied while rows represent unique objects. TRUEFALSE\nCases typically represent what we are studying while variables typically represent who or what we are studying. TRUEFALSE\nIn a dataset tracking the commuting preferences of individuals in Groningen, some relevant variables might include the mode of transport they use, their age, and their gender, while cases might capture different individuals at one (or multiple) points in time. TRUEFALSE\nIn a dataset tracking the commuting preferences of individuals in Groningen, some relevant cases might include the mode of transport they use, their age, and their gender, while variables might capture different individuals at one (or multiple) points in time. TRUEFALSE\n\n\n\n\nQualitative and Quantitative Data\nNot all variables look the same, and this leads us to a key distinction between qualitative and quantitative data. Quantitative data is measured with numbers, such as income, exam scores, or height. Some of these numbers are discrete, meaning they can only take on whole-number values—like the number of children in a household, or the number of books someone has read this month. Others are continuous, meaning they can take on any value within a range, like height, income, or time spent commuting.\nThis distinction matters because it affects how we summarize and analyze the data. For example, with discrete variables, counts and frequencies are often natural summaries, while continuous variables often require measures like averages, ranges, or percentiles.\nQualitative data, on the other hand, refers to categories or descriptions, such as marital status, eye color, or whether someone prefers tea or coffee. This distinction is important because it determines what kinds of analysis are appropriate. You can calculate averages and percentages for quantitative variables, but those operations make little sense for qualitative data. For qualitative variables, frequencies or proportions are often more useful.\nAlthough qualitative data is not inherently numerical, in practice we often assign numbers to categories so that we can analyze them statistically. For example, marital status might be coded as 1 = single, 2 = married, 3 = divorced. Similarly, survey responses like “strongly disagree” to “strongly agree” might be assigned values from 1 to 5. These numbers don’t turn the data into true quantities — they are simply labels that make the data easier to store, summarize, and compare. But the meaning of the numbers depends entirely on the type of variable we are working with, which brings us to the standard classification of data types: nominal, ordinal, interval, and ratio.\n\nBut before we get to that, why not stop and test yourself to ensure that you truly understand what you are reading? Indicate whether each of the statements below is TRUE or FALSE.\n\nThe mode of transport people use to travel to work is a quantitative variable. TRUEFALSE\nThe typical house/apartment number in a regular neighbourhood is a continuous variable. TRUEFALSE\nPost Codes are discrete quantitative variables. TRUEFALSE\nPost Codes are qualitative variables. TRUEFALSE\nTyre pressure is a continuous variable. TRUEFALSE\n\n\n\n\nData Types: Nominal, Ordinal, Interval, Ratio\nNominal data consists of categories without any natural order, such as blood type or favorite fruit. Ordinal data adds an order to the categories, but the spacing between them is not consistent. For example, a satisfaction scale from “poor” to “excellent.” Interval data is numeric and has meaningful spacing between values, but lacks a true zero point. We say something lacks a “true zero” when a value of 0 does not indicate absence of said variable, such as a temperture reading of 0 degrees celcius - you would never say that this means there is no temperature. Ratio data includes all the properties of interval data, but with a true zero, such as height, distance, or income. This last type allows for meaningful ratio comparisons, like saying someone who earns £40,000 makes twice as much as someone earning £20,000, something not possible with interval data, or, more precisely, something not possible when the data lacks a true zero.\n\n\n\n\n\n\nNote\n\n\n\nRatio and interval scales both measure quantities, but there is a key difference: the zero point. Interval scales, like temperature in Celsius or Fahrenheit, have an arbitrary zero — 0°C does not mean “no heat” — so while we can measure differences, we cannot meaningfully say one value is twice another. Ratio scales, like weight or height, have a true zero, which represents a complete absence of the quantity. This absolute zero allows us to make multiplicative comparisons: an object weighing 10 kg is meaningfully twice as heavy as one weighing 5 kg. In short, a true zero provides a fixed anchor that makes ratios and proportional statements meaningful, whereas an arbitrary zero allows only for comparisons of differences.\nAdmittedly, while it is important to understand the theoretical differences between interval and ratio data, this distinction rarely comes into play in practice.\n\n\nThese distinctions may sound abstract, but they matter a great deal in practice. The type of data determines which statistical methods are valid. You can calculate averages for ratio or interval data, but not for nominal data. You can rank ordinal data, but you cannot assume the differences between ranks are equal. Confusing these categories can lead to misleading results. Equally, ignoring how data was collected or structured can mean placing trust in biased or incomplete information.\n\nBefore moving further, try to correctly answer the following questions by selecting the most appropriate data type for each scenario.\n\nWhat type of data is “Blood type (A, B, AB, O)”? ordinalnominalintervalratio\nWhat type of data is “Customer satisfaction rating on a 1–5 scale”? ordinalnominalintervalratio\nWhat type of data is “Temperature in Celsius”? ordinalnominalintervalratio\nWhat type of data is “Annual income in dollars”? ordinalnominalintervalratio\nWhat type of data is “Ranking of students by exam score (1st, 2nd, 3rd, etc.)”? nominalordinalintervalratio\nWhat type of data is “Number of pets someone owns”? ordinalnominalintervalratio\nWhat type of data is “Cuisine preference ranking (e.g., Italian, Mexican, Japanese)”? nominalordinalintervalratio\nWhat type of data is “IQ score”? ordinalnominalintervalratio\nWhat type of data is “Clothing size (S, M, L, XL)”? nominalordinalintervalratio\nWhat type of data is “Distance run in kilometers”? ordinalnominalintervalratio\n\n\nIn short, data is never just data. Its source, structure, and type all shape what we can learn from it. By paying attention to how data is collected, how it is organized, and what kind of information it contains, we can ask better questions, avoid common mistakes, and make stronger, more reliable conclusions."
  },
  {
    "objectID": "posts/diff_data_1.html",
    "href": "posts/diff_data_1.html",
    "title": "Different Types of Data: Part 1",
    "section": "",
    "text": "What Is Data? A Continuation\nData describe the universe we wish to study. There are no ends to the universes which can be studied, and therefore the universes represented by data.\nData is everywhere. From the number of emails in your inbox to city traffic counts, data help us make sense of the world. But not all data are the same. Understanding the types of data, where they come from, and how they’re documented is essential for using them effectively.\n\n\nSources of Data: Internal vs. External\nAt the most basic level, we distinguish between data that already exist in some form, from data that we propose to collect ourselves in the course of our research. Data are said to be internal when they are available in some form through existing records/files of an institution undertaking the study. That is, the data are already available to you internally and thus you do not have to collect it. With this in mind, a key characteristic of internal data is that the researcher knows an awful lot about how the data was collected and what they are measuring. Examples include sales figures, employee records, or website analytics. These data are often easier to access and tailor-made for your (organisation’s) specific needs.\nData are said to be external when they are obtained from an external entity/organisation. That is, the data are coming from outside sources. In these situations, many important characteristics about the data (i.e., how they were collected and measured) may not be known. Examples include census data, market reports, or climate records from government agencies. External data can enrich internal analysis by providing context or allowing comparisons across organizations or regions.\nBoth internal and external data can be valuable, but they may differ in format, quality, and availability. Understanding the source helps determine how much you can trust it and how it can be used. Indeed, it is not unusual to derive results in a statistical analysis that cannot be explained without detailed knowledge of the data source.\nBecause internal data is often tailor-made to suit a specific objective, many institutions, researchers, and organisations collect what are otherwise very similar data but are both measured and used in different ways. One obvious example here concerns geographical statistics (i.e., urbanisation) across countries. Many countries have different definitions for cities and urban areas, thus making measurements of urbanisation very different. Yet most are interested in measuring urbanisation in some sense. The differing objectives and approach to collecting such data raises immediate issues for researchers and organisations interested in, say, comparing urbanisation levels across countries. At times, statisticians are called upon to make comparisions across countries, in this case, for which data collection procedures are different, data accurracy differs, and even data definitions differ - hence the considerable resources dedicated by organisations like the UN to gather and integrate such disparate data sources. With this in mind, caution should always be exercised in the use of external data.\n\nTo test your understanding of the different types of data, try to answer the following questions:\n\nYou work in the Sales Department of Albert Heijn’s corporate offices and wish to study the profit margins of the in-store bakeries across the Netherlands. The data you are using is … InternalExternalNeitherA Mix of Both\nYou work for the University of Groningen and wish to use CBS microdata to study wage distributions across different Dutch municipalities. The data you are using is … InternalExternalNeitherA Mix of Both\nYou want to study how your personal expenditures on groceries have changed over the past twelve months. To this effect, you have kept all of your receipts and wish to conduct some statistical analysis. The data you are using is … InternalExternalNeitherA Mix of Both\n\n\n\n\nPrimary Versus Secondary Data\nAnother useful distinction is between primary and secondary data. Primary data are data collected firsthand by a researcher, organization, or system for a specific purpose. They come directly from the original source. Some examples include: survey responses, experiments, observations, sales transactions. Secondary data are data that were collected by someone else, often for a different purpose, and are later reused for analysis. Some examples include: census statistics, published research datasets, government or NGO reports.\nInternal data are always primary because they are collected firsthand within the organization for a specific purpose. The advantage of primary data is that you have control over the process, measurement, and definitions. The downside is that collecting primary data can be costly and time-consuming.\nSecondary data, by contrast, are collected by someone else, often for a different purpose, and are reused for your analysis. Examples include government statistics, academic datasets, or industry reports. Secondary data are usually easier and cheaper to access, but you must critically evaluate whether they are appropriate, reliable, and up-to-date for your questions.\nWhen using external data, it is important to always important to get as close to the primary source as possible. The difficulty with secondary sources is that they may contain data that has been altered by recording or editing errors, selective data omission, rounding, aggregation, questionable merging of datasets from different sources, or various ad hoc corrections. For example, never use an encyclopedia to get a list of the 10 largest cities in Europe; use the data collected by Eurostat - the statistics agency dedicated to collecting consistent socio-economic statistics across European countries.\nOften, good research combines both: using secondary data for context or historical perspective, and collecting primary data to answer specific, current questions.\n\nTo test your understanding of the different types of data, try to answer the following questions:\n\nYou wish to study sentiments of stock markets before, during, and after financial crises. You reason that reports from leading newspapers of the time offer reasonable approximations of how society felt about financial markets. So you read through these articles and generate an index from the information provided. The type of data you are using is … PrimarySecondaryNeitherA Mix of Both\nYou are studying student housing affordability in Barcelona, Spain, and conduct a survey at the Univeristy of Barcelona which asks questions concerning things like income, rental prices, and living arrangements. The type of data you are using is … PrimarySecondaryNeitherA Mix of Both\nYou are studying the evolution of property prices in England and Wales since the Great Recession and use the property price index developed in Ahlfeldt, Carozzi, and Makovsky (2023) to do this. The data you are using is PrimarySecondaryNeitherA Mix of Both\nYou are studying labour market outcomes across the United States and use U.S Census data to do so. The data you are using is PrimarySecondaryNeitherA Mix of Both.\n\n\n\n\nMetadata: Data about Data\nFinally, any serious discussion of data types is incomplete without mentioning metadata.\nMetadata are data about data. They describe how, when, where, and by whom the data were collected, as well as definitions, units, and any limitations. For example, a dataset of school test scores might include metadata detailing the grade level, the subjects tested, the testing dates, and how missing values were handled.\nMetadata are essential for interpreting and reusing data correctly. Without metadata, even well-collected data can be confusing or misleading. Think of metadata as the instruction manual for your dataset — it tells you what the numbers or categories really mean and how to use them responsibly. They are usually presented in document (i.e., pdf) or spreadsheet (i.e., excel) format, and if they do not come directly with the data themselves, will usually be readily available from whatever source they come from.\nAny dataset worth its salt contains some form of metadata. Admittedly, some will be better than others, but nearly all modern datasets have something to work from.\n\nConsidering the characteristics of metadata, answer the following question:\n\nMetadata are readily available with primary data.\n\n No, not unless you make metadata to accompany whatever primary data you have collected. Yes, any dataset worth its salt contains some form of metadata. Therefore, every dataset will come with some No. It is impossible to have metadata for primary data since you must collect the data yourself\n\n\n\nData can be distinguished in several ways. Source (internal or external) and collection method (primary or secondary) are two ways which we have explored today. But there are others, namely by nature, something that will be explored in my next post.\nAlongside the data itself, metadata provides essential documentation about its structure, meaning, and quality. Recognizing these distinctions helps you select the right data, apply it effectively, and interpret it responsibly; because data is powerful only when we understand where it comes from, how it was collected, and what it truly represents.\n\n\nBibliography\n\nStatistics: A Very Short Introduction, by David J. Hand\nElementary Statistics For Geographers by James Burt, Gerald Barber, and David Rigby"
  },
  {
    "objectID": "posts/what_is_stats.html",
    "href": "posts/what_is_stats.html",
    "title": "What Is Statistics?",
    "section": "",
    "text": "Thinking Statistically: A Refresher\nStatistical thinking begins with something very simple: noticing. We notice events, we notice people, we notice differences. We see rents climbing in our city, or hear friends complain about how much they’re paying, and we wonder: is this just a few unlucky cases, or does it say something bigger about the housing market? This is the heart of thinking statistically. It is not about complicated formulas, but about moving from what we observe to what we might reasonably believe about the world. The post Thinking Statistically: A Primer goes into more detail on this point.\n\n\nFrom Observations To Patterns\nSometimes, we observe a single thing — a person, a place, an event — and note several of its features. More interestingly, we often observe many things that are similar in some respects but quite different in others. We notice patterns across people, places, or time. We end up with a collection of observations, or, in the language of statistics, data.\nFaced with such a collection, it is natural to start comparing. What is the same? What is different? What might explain those differences? This is where statistical thinking deepens. The overarching question becomes: what can we learn from this data?\nOne of the great temptations in everyday life is to leap from a vivid example to a sweeping claim. A frustrating encounter with a GP becomes “the NHS is broken.” A handful of pricey flat listings becomes “this city is unaffordable.” But while anecdotes grab attention, they rarely tell the whole story. If we want to understand whether a city is truly unaffordable, for example, we need more than isolated cases: we need patterns.\nThat means gathering many observations and looking for regularities across them. Instead of just noting that a friend’s rent has doubled, we might ask: what are average rents across the city? How do they compare to incomes? How do they vary between neighbourhoods or over time? These “how much” questions — how much bigger, how much more frequent, how much more difficult — are everywhere in daily life, and they are precisely where statistical thinking deepens. We move from anecdotes to collections of observations, and from collections to evidence that can support or challenge our claims.\nThis is why statistics is best understood not as a tool for eliminating uncertainty, but as a way of working productively with it. The numbers never speak for themselves. They help us move from stories to systems, from isolated cases to general patterns.\n\n\nLikelihood and Certainty\nEven with careful observation, we rarely get perfect answers. Instead, we learn to talk in terms of likelihood. At its core, statistical thinking is about likelihood, not certainty. We weigh the chances (i.e., probability). We expect patterns to hold “in general” or “on average” or “in the long run,” but it never promises that every case will follow the rule.\nThis is what makes statistics so powerful for the social sciences. People, places, and societies are complex and unpredictable. But with good data and careful reasoning we can still learn a lot. We can be clear about what we know, open about what we don’t, and realistic about how confident we should be.\nStatistics helps us summarize what we have found so we can be clear about the facts. But it also helps us go further: to compare, to explain, and to predict. It reminds us to be cautious. What appears true in one context may not hold in another, as I am sure you are all acutely aware. Before we can test a hypothesis or quantify uncertainty, we need to know what matters, what varies, and what might be related. This requires a theoretical understanding of the problem you are studying just as much as it requires an understanding of what is happening in your data.\nIf we want to generalize more confidently, the logical starting point is to gather more experience - that is, collect more data. This speaks to the fact that the more contexts, situations, and experiences we observe, the more confident we can be in drawing conclusions and making suggestions. In other words, if we want to generalize more confidently, we need more data, which happens to be the logic underpinning one of the most powerful ideas in statistics: the Central Limit Theorem.\nThinking statistically then, is not about memorising formulas or crunching numbers. It is about developing a habit of mind: noticing variation, looking for patterns, and reasoning in terms of likelihood. Done well, it sharpens our instincts and grounds our judgments. It helps us move beyond stories to understanding, and beyond uncertainty to insight.\nIn that sense, statistics does not replace your instincts. It organizes and strengthens them. You already ask whether something seems off. You already notice patterns. What statistics offers is a way to build on those instincts: to sharpen them, test them, and sometimes correct them. It helps you become more precise in your questions and more careful in your answers. And that is a powerful skill, no matter what field you are in.\n\n\nWhat Is Statistics?\nStatistics is a word that causes confusion, not because it is complicated, but because it has so many meanings. It can refer to a subject of study, to methods of analysing data, to the data themselves, or to the specific numbers we calculate. For example, a researcher might study statistics, use statistics to analyse data, interpret official statistics, and report a statistic like average income. This is partly why, in our statistics courses, we tend to adopt the following definition of statistics:\n\n\n\n\n\n\nImportant\n\n\n\nStatistics is the methodology for collecting, presenting, analysing, and interpreting data.\n\n\nMany students are introduced to statistics so that they can interpret and understand research carried out in their field of interest. But statistics is everywhere — in the news, in sports, in science, in policy, in medicine. Yet it is often misunderstood. Many people see it as dry, overly technical, or even manipulative. Some fear it as a tangle of formulas; others distrust it because numbers can be twisted to tell a story. This suspicion around statistics is understandable. We’ve all seen numbers used to persuade, distract, or deceive. Sometimes statistics are gamed — when a measure becomes a target, it can lose its meaning (Goodhart’s Law). But the problem is rarely the statistics themselves; it’s how they are used, what is being measured, and how we interpret the results.\nIn fact, modern statistics is less about arithmetic and more about reasoning, exploration, and interpretation. Computers handle most calculations today, freeing us to focus on the questions we ask, the assumptions we make, and the conclusions we draw. In that sense, statistics is a technology — a set of tools for extracting meaning from data, for navigating uncertainty, and for learning from evidence.\nAt its core, statistics embraces the imperfection of our observations. We know that any single measurement is flawed; our data are simplified representations of a complex world. Statistics gives us ways to measure and reason through these uncertainties, to understand variation, and to weigh how far wrong we might be. Hence why good statistical thinking matters.\n\n\n\n\n\n\nNote\n\n\n\nThis is why data and statistics are inseparable. Data, often numerical but not always, allow us to represent what we are studying. They are never perfect, but they are workable — and with good judgement, they let us move from noticing patterns to understanding systems. Poor data, of course, lead to poor conclusions irrespective of the tools used.\n\n\n\n\nBibliography\n\nStatistics: A Very Short Introduction, by David J. Hand\nStatistics Without Tears: An Introduction For Non-Mathematicians, by Derek Rowntree\nElementary Statistics For Geographers, by James E. Burt, Gerald M. Barber, and David L. Rigby"
  },
  {
    "objectID": "posts/school_run.html",
    "href": "posts/school_run.html",
    "title": "Start Young, Drive Less: The Overlooked Power of the School-Run",
    "section": "",
    "text": "School-runs are a common but underexamined part of daily mobility. In Ireland, they account for nearly one in five household trips, yet they’re often treated as an extension of the adult commute or folded into broader discussions about household logistics. Our recent study aimed to take the school-run seriously; not just as a variant of commuting, but as a travel behaviour shaped by a distinct set of social, spatial, and infrastructural dynamics.\nDrawing on census data from almost 300,000 secondary school students across Ireland, we linked individual travel behaviour to a range of built environment characteristics. We used Generalised Structural Equation Modelling (GSEM) to capture the direct and indirect relationships between factors like land-use mix, transport infrastructure, household structure, and mode choice. This approach allowed us to unpack how both context and constraint shape how students get to school.\n\nSchool-Runs Are Not Just Commutes with Backpacks\nIn some respects, school-runs mirror adult commuting: students are more likely to walk, cycle, or take public transport when they live closer to school, in more compact neighbourhoods, and in areas with better infrastructure. These patterns align closely with findings from commuting research.\nBut school-runs also reflect the complexity of family routines. Even for secondary students, decisions about how to get to school are influenced by household structures, parental schedules, perceptions of safety, and the presence of siblings. Car-based trip-chaining is common, and family habits around transport tend to carry over across generations.\nRecognising these dynamics helps us move beyond individual preferences and towards a better understanding of the constraints and contexts that shape school travel.\n\n\nWhat We Found\nOur findings can be summarised around three key insights:\nBuilt environment matters. Students living in more mixed-use, compact areas with better walking and transit infrastructure are significantly more likely to use sustainable modes. These effects hold even when we account for household income and other socioeconomic factors.\nDistance remains the strongest determinant. Longer school trips are strongly associated with car use. Students who live and study in the same neighbourhood, especially within the same Electoral Division — are much more likely to walk or take public transport. Travel distance continues to shape the choice set in powerful ways.\nHousehold context matters. Students from more affluent and two-parent households are more likely to be driven to school. While these households may live in areas with good infrastructure, access to a private vehicle seems to override other factors, highlighting the role of habit, convenience, and household coordination.\nWe also found that the structure of the local transport network plays a key role. Areas with denser road networks tended to see higher rates of car use, while greater availability of walking and cycling infrastructure was associated with more active travel. In short, people tend to use the infrastructure that’s available to them.\n\n\nWhy It Matters\nThere is growing policy interest in promoting active travel and reducing car dependency for health, environmental, and congestion reasons. But most interventions still focus on adult commuters. The school-run, by contrast, offers a more consistent and replicable point of intervention. It happens daily, follows predictable routes, and shapes the travel habits of young people.\nWhile behaviour change campaigns are important, our findings suggest that structural conditions, particularly built form, infrastructure quality, and travel distance, remain central to shifting how people move. If we want to support more sustainable school travel, we need to plan for it: shorten distances between home and school, build coherent and safe active travel networks, and improve the quality and reliability of public transport.\nUltimately, school-runs are not just about getting from A to B. They reflect broader spatial, social, and institutional dynamics. Understanding them better can help us build cities that support more inclusive and sustainable forms of everyday mobility.\nFor the full technical details, please check out the full paper here!"
  },
  {
    "objectID": "posts/get_started_r.html",
    "href": "posts/get_started_r.html",
    "title": "Getting Started With R and RStudio",
    "section": "",
    "text": "R is a powerful and flexible programming language used for data analysis, visualization, and statistical modeling. Coupled with RStudio, a feature-rich integrated development environment (IDE), it provides a smooth workflow for beginners and experienced users alike. This guide will walk you through getting started with R and RStudio, from installation to working with scripts, Quarto documents, and R projects; all of which are essential for the successful completion of computer practical assignments and research projects alike.\n\nInstalling R\nBefore you can start coding in R, you need to install two pieces of software: R, the programming language itself, and RStudio, the IDE that makes working with R easier. You must install them in this order. R itself is a command-line language, so if you were to run it without an IDE, you would interact with it in a basic console. This works, but it lacks many productivity features such as syntax highlighting, project management, and integrated help that RStudio provides. Thus, you can think of R as a car’s engine while RStudio is like a car’s dashboard.\nStep 1: Installing R \nNavigate to https://cran.r-project.org/ where you can find various versions of R. On top, there is a list of 3 links redirecting you towards the most appropriate R version for you, depending on your operating system. Then, click on the link which is suitable for your operating system.\nIf you work on Windows, click on install R for the first time. And then click on Download R-X.X.X for Windows on top (R-X.X.X indicates the current version of R, which at the time of writing is R-4.4.2). Your browser will start downloading the installation file for R and, once downloaded, start the installation of R (you can safely click on Next on all the default settings).\nIf you work on macOS, select the R version which is suitable for your operating system. Your browser will start downloading the installation file for R and, once downloaded, start the installation of R (you can safely click on Next on all the default settings). You will also have to install XQuartz for some libraries in R to work, so do that before you advance further.\n\n\n\n\n\n\nNote\n\n\n\nIf you work on Linux, installing R can be a bit awkward becuase involves using your distribution’s package manager. Therefore, the exact commands depend on whether you are using Ubuntu/Debian, Fedora, or another distribution. For Ubuntu or Debian-based systems, open a terminal and run: \nsudo apt update  sudo apt install r-base \nThis installs the base R system. You can verify the installation by typing: R --version. \nFor Fedora or Red Hat-based systems, you can use: sudo dnf install R \nOnce installed, you can start R in the terminal by typing R.\n\n\n\n\nInstalling RStudio\nOnce R is installed on your computer, navigate to https://posit.co/download/rstudiodesktop/ to download RStudio Desktop. Click on Download RStudio Desktop for X, where X is your operating system. The webpage automatically recognizes your operating system and suggests the correct version of RStudio to be downloaded.\nYour browser will start downloading the installation file for RStudio and, once downloaded, start the installation of RStudio (you can safely click on Next on all the default settings). When running RStudio for the first time, the software will ask you to select an existing R version installed on your computer, select the one you installed in the previous point.\n\n\n\n\n\n\nNote\n\n\n\nFor Linux users, the process is, once again, a bit more complex. \nYou download the .deb (for Debian/Ubuntu) or .rpm (for Fedora/Red Hat) installer from https://posit.co/download/rstudiodesktop/. \nOnce downloaded, open the terminal and navigate to the download location and run: \nsudo dpkg -i rstudio-x.yy.zzz-amd64.deb (replace x.yy.zzz with the actual version number) \nOn Fedora, download the .rpm file and run: \nsudo rpm -i rstudio-x.yy.zzz-x86_64.rpm \nAfter installation, you can launch RStudio from your application menu, just like on Windows or macOS. The interface and workflow are identical across platforms, so once installed, you can follow the same instructions for navigating the console, scripts, and Quarto documents.\n\n\nIn case these instructions are difficult to follow and/or raise questions, you can consult the following videos which offer helpful step-by-step guides in installing R on your device.\nFor Windows: https://www.youtube.com/watch?v=TsnGd6p9oTk  For macOS: https://www.youtube.com/watch?v=CICcOLgKxl0  For Linux: https://www.youtube.com/watch?v=7RSq4uuS36A \n\n\nIntroduction to RStudio and its Environment\nAfter installation, open RStudio. You will see a layout divided into three main panes by default:\n\nConsole (left): This is where you can type and run R commands directly. For example, if you type 1 + 1 into your console and click enter, you will see the answer 2 emerge.\n\n\n1 + 1\n\n[1] 2\n\n\nThe console is great for quick calculations, testing small code snippets, and exploring data interactively.\n\nGlobal Environment/History (top-right): The Global Environment pane displays all objects (variables, datasets, functions) that exist in your current R session. This is basically our workspace. Any variables that we define, or data that we import and save in a dataframe, are stored, and listed in our Global Environment. For example, if you type into your console\n\n\nresult &lt;- 1 + 1\n\nand hit enter, you will see a new object appear in your environment tab called result. The value of this object will be the sum of 1 + 1, because that is what we have told R it should be. We do this using the &lt;- convention. This helps us to generate a discrete output using a series of inputs, and forms the foundation of basically all commands in R. In other words, we use the &lt;- to tell R to create a new object called result which is made up of the sum 1 + 1. This new object will be stored in your global environment for you to use, should you wish to do so, until you close RStudio.\nTheoretically, your global environment can store many objects. But we are usually not interested in everything, and sometimes we create “temporary objects” which might be only for testing or whose primary purpose is to be used as an intermediary input. If you want to tidy up this environment, you can do so very easily. Simply click the broom icon at the top of the window, and that will remove every object. If you want to remove only some elements, select the Grid view from the dropdown menu, then you can check the boxes of the objects you would like to remove and use the broom icon to remove them.\nWithin this area, you can also import data from various sources, and you can access the History tab, which lists the commands that have been used previously.\n\nThe area in the bottom-right: The area on the bottom-right contains multiple tabs, such as files, plots, packages, help, and viewer.\n\nThe Files tab shows the files and directories that are available within the default workspace of R. The Plots tab shows the plots that are generated during the current session. The Packages tab helps you to look at what are the packages that are already installed in RStudio, and it gives a user interface to install new packages (more on this in another post). The Help tab is the most important one where you can get help from the R Documentation on the functions that are in built-in R. The final and last tab is the Viewer tab which can be used to see the local web content that’s generated using R.\nTo familiarize even further on the RStudio environment, watch the following video and replicate what is shown in the video in your own RStudio session on your computer: https://www.youtube.com/watch?v=FIrsOBy5k58\n\n\nDifferent File Types in R\nR code can be written and executed in one of two places (normally). The console is where you execute code line-by-line, but not saved. So, if you run code in this pane, it is impossible to save it without transferring it to another file/place first. Consequently, working solely from the console is not advised. If we want to be able to reproduce and reuse our code for further needs, we should write it in a Script file rather than directly in the Console. To start recording a Script, click File &gt; New File &gt; R Script. This will open a text editor in the top-left corner of the RStudio interface (above the Console tab, which moves the console tab to the bottom-left).\nR scripts are plain text files containing a series of R commands and code that execute specific tasks or analyses. These scripts typically end with the .R extension and can include anything from data manipulation, statistical analysis, visualization, to machine learning algorithms. They can be run interactively within an R environment or executed in batch mode for larger tasks. By organizing code into scripts, users can save, modify, and share their work easily, making them a fundamental part of R programming. The idea is that you can open this file at any time, in any place, and execute the same code as you did originally.\nWhile writing a Script, it’s a good idea to add comments (using the # symbol followed by a line of comment text) to explain the purpose behind certain pieces of code to a future reader. Additionally, it’s helpful to provide important context at the beginning of the Script, such as the author and contributors of the code, its creation and modification dates, scope, etc. Another useful practice is to load all the necessary R packages at the start of the Script, following the initial information.\nQuarto documents are the modern equivalent of R Markdown. They allow you to combine R code, narrative text, equations, and visualizations in a single document. Quarto is ideal for creating reports (like those in the CP assignments), blog posts (like the one you are currently reading), or academic papers where you want both analysis and narrative integrated seamlessly. A simple Quarto code chunk looks like the code examples you have seen earlier in this post.\nYou can create other types of files in R, such as presentation files and interactive dashboards, but the fundamentals lay with scripts and quarto documents. Indeed, the website on which this blog post is hosted has been created using a combination of script files, quarto documents, and one R project, illustrating how far you can go with a good understanding of these files.\n\n\nGetting Started With Quarto\nQuarto is a tool which allows you to create documents that combine text, code, and results from running that code in R. It’s kind of like a recipe book, where you can write down instructions for doing some analysis or creating a graph, and then also include the code and the output of that code right alongside the text. This makes it easy for others to follow along with what you’re doing and reproduce your results.\nWith Quarto, you can create documents that include code written in multiple programming languages, not just R. You can also use different types of outputs, like HTML, PDF, or Word documents, and you have more control over the formatting and styling of your document.\nOverall, Quarto is a tool that helps you create documents that combine text, code, and output in a way that’s easy to read and share with others. It is great for doing data analysis or creating reports, and it can make your work more transparent and reproducible.\nWatch the following videos and replicate what is shown in these videos in your own RStudio session on your computer:\n\nInstall Quarto on your computer: https://www.youtube.com/watch?v=4rKWIJXe3mA&list=PLEzw67WWDg80-fT1hq2IZf7D62tRmKy8f \nIntroduction to Quarto (part 1): https://www.youtube.com/watch?v=31Q9ZTZOHIM&list=PLEzw67WWDg80-fT1hq2IZf7D62tRmKy8f&index=2 \nIntroduction to Quarto (part 2): https://www.youtube.com/watch?v=QlYgnf_qCNo&list=PLEzw67WWDg80-fT1hq2IZf7D62tRmKy8f&index=3 \n\nGetting started with R and RStudio involves more than just installing software. Understanding the layout of RStudio and the types of files you can work with will set you up for a productive workflow. Scripts allow you to save and organize your code and Quarto documents enable reproducible reports and dynamic documents. In my next post I will introduce R Projects, which act as the scaffolding that keeps everything structured and isolated.\nOnce you are comfortable with these basics, you can start exploring datasets, creating visualizations, and performing analyses with confidence. As you progress, RStudio’s rich features — including debugging tools, integrated help, and package management — will make it easier to tackle increasingly complex data challenges."
  },
  {
    "objectID": "posts/commute_costs.html",
    "href": "posts/commute_costs.html",
    "title": "What is the cost of commuting?",
    "section": "",
    "text": "It’s well known that where you live influences how you travel. A recent study, co-authored with Dr. Kevin Credit takes a different approach by exploring how where you live impacts the cost of travelling. More specifically, we look at how different types of built environments shape the costs of using particulary modes of transport, and the costs of travelling along specific routes, when commuting in the Dublin metropolitan area.\n\nUnderstanding Travel Costs\nAt the heart of our study is the concept of mobility costs—a way of capturing how much effort, time, and inconvenience it takes to commute using different modes of transport. These costs aren’t just financial; they reflect how infrastructure, congestion, and land-use patterns shape the real-world effectiveness of getting from A to B. Rather than looking only at which transport modes people choose, we focus on how cost-effective those modes are in different local environments. Using a “cost-distance” approach, we estimate the relative efficiency of car travel, walking, cycling, and public transport across the Dublin region. This allows us to uncover how built environments influence travel indirectly, by making certain modes more or less viable in different areas.\nDrawing on 2016 Census data and sources like Google Maps, OpenStreetMap, and the Irish Revenue Commissioners, we link commuter flows to detailed transport networks. We then use random forest models, a type of machine learning, to uncover how local environments shape mobility costs—often in complex, non-linear ways.\n\n\nWhat we found\nActive travel (walking and cycling) is most cost-effective in central Dublin and denser, walkable areas—but this benefit fades in overly dense or congested areas. We link this to the fact that these characteristics, while initially beneficial, can eventually lead to overcrowded streets, longer wait times at crossings, and more frequent stops. In these settings, travelling on foot or by bike may become stressful, when compared to other modes.\nSurprisingly, public transport is also quite inefficient in very dense areas. One might expect that higher density would support better service, shorter wait times, and greater usage. However, we suspect this result emerges because, in Ireland, road space is shared between cars and buses. The result? Cars and buses get stuck in the same traffic, making the trade-off between cars and public transport one which revolves around convenience and reliability—trade-offs cars win every time given the lack of countervailing measures (e.g., congestion pricing) in Irish city centres.\nThese patterns highlight how urban form and infrastructure design shape the real cost of commuting—not just in money, but in time, effort, and convenience.\n\n\nWhy this matters\nIf cities want to encourage sustainable travel, they need to reduce the relative cost of walking, cycling, and public transport. That means investing in segregated infrastructure, like bus and bike lanes that bypass congestion. But it also means designing coherent, connected transport networks, and thus directly influencing the viability of different transport modes across the urban-rural continuum.\nOne key observation we make is that Dublin’s public transport and cycling networks may be lagging behind those of global cities. For public transport, this observation stems from the lack of segregated transport infrastructure, while for active transport, it stems from the fragmentation of networks.\nProjects like BusConnects are steps in the right direction. But unless they’re supported by smart land-use planning that curbs sprawl, their impact will be limited. Simply put: we can’t build our way out of car dependence without also re-shaping the environments people move through.\nFor policymakers and urban designers, the message is clear: making sustainable travel easier isn’t just about offering alternatives: it’s about making those alternatives cost-effective, reliable, and competitive with the car.\nFor the full technical details, please refer to the full paper."
  },
  {
    "objectID": "posts/dataset_types_r.html",
    "href": "posts/dataset_types_r.html",
    "title": "Datasets and Data Types As Seen in R",
    "section": "",
    "text": "Understanding Data Types in R: From Nominal To Ratio\nWhen we begin working with data in R, one of the first challenges is learning how to think about the different types of variables. In statistics, we often distinguish between four levels of measurement: nominal, ordinal, interval, and ratio. These levels aren’t just technicalities. They determine what we can meaningfully do with our data, and just as importantly, what we cannot do. R doesn’t enforce these distinctions for us, but it does represent variables in certain ways (i.e., as numbers, factors, or characters) that shape how we interact with them.\nTo make these ideas more concrete, let’s start by loading a dataset. R comes with several datasets built in (sometimes called “data frames”), which makes practice very easy. One of the most famous is the iris dataset, which contains measurements of sepal and petal dimensions for 150 flowers across three species. In your R Console, you can type help(iris) if you want more information on the dataset.\n\n\n\n\n\n\nTip\n\n\n\nR comes with extensive built-in documentation for every function, dataset, and object. The help() function is the primary way to access this documentation. For example, running help(iris) or ?iris in the console brings up a page describing the iris dataset: how many rows and columns it has, what each variable represents, and sometimes even references to the original source. Using help() is essential when you are exploring a new dataset or learning a new function, because it provides the authoritative explanation of how the object is structured and what operations make sense. It also shows examples of usage, which is invaluable when you are experimenting in R for the first time.\n\n\n\n\nLoading in a dataset\nWhen we are working with any dataset in R, the first task is to load it into our R environment. Typically, we do this using the load()command. But because iris is comes built into R, we do not need to do this, nor do we have to downlaod anything. All we have to do is type data(iris) to make it available in our session.\n\n#Call the dataset that we would like to use\ndata(iris)\n\nOnce called, you should see the iris dataset appear in your environment tab. Its name should be iris and you should see the label &lt;Promise&gt; next to it. This signals that the dataset is ready for use.\n\n\n\n\n\n\nNote\n\n\n\nThis &lt;Promise&gt; and activation step only happen when working with R’s built-in datasets. When we load external data (i.e., datasets used in the CP assignments), they become instantly available for use and manipulation.\nIn this case, the iris dataset is also instantly available for use and manipulation, you just have to explicitly do something with the dataset for it to become activated. For example, if you hover your mouse over the iris object and click on it, you will see that the &lt;Promise&gt; label disappears and is replaced with […]. Similarly, if you run any form of command, as we will do shortly, using the dataset in question, then it will automatically activate.\n\n\nOnce loaded into R, we can do a whole host of things to the dataset. One recommended starting point is to create a carbon copy of the raw data you are using. This can be helpful if you plan on manipulating the data or changing it in some way. It is always advisable to keep an unaltered copy of the raw data on hand in case you make a bags of it and need to start over - if you alter the only copy of your raw data and it turns out to be incorrect, you could land yourself in hot water.\n\n#Generate a copy of our dataset and call it iris_data\niris_data &lt;- iris\n\nAll we have done here is created an identical object but given it a new name. In R-speak, what we have done is, using the object iris, we created a new object called iris_data. The direction of the arrow illustrates which object is an input of the function, and which is an output. For example, we could also do it this way:\n\n#Flip the sign of the arrow\niris -&gt; iris_flip\n\nIt is, however, much less typical to structure your code this way because there are normally far more inputs than there are outputs. So having your ouptuts on the left-hand side of the arrow (i.e., iris_data &lt;- iris) allows you utilise the full breadth of the document when constructing code and commands.\n\n\nExploring the structure of datasets\nOkay. Enough about loading datasets in. If we want to explore our dataset in more detail, we can go about it in a few ways. On one hand, you can use your mouse and simply hover over your dataset, click on it, and navigate the table that automatically opens. This table is your data. Each cell represents a data point. In most datasets, the rows will represent cases; so that, in this case, each row represents a different flower. Meanwhile, the columns will usually represent variables - that is, the different characteristics of each flower that this dataset captures.\nIn this case, for example, we see that there are five variables (characteristics) in total and 150 observations (flowers) in total. We can confirm this in the table, but also in the environment tab, as this information now replaces the &lt;Promise&gt; label. For each of these 150 flowers, we have information about their petals and sepals, as well as the type of iris species they belong to. We see that, for four of these variables, their values are expressed as numbers, whereas for the fifth, its values are expressed in words.\nThis is about the most we can gather from this table without having to make educated guesses about the underlying structure of the data. To dig a bit deeper we will have to use some code to examine the structure of the dataset and its different variables. Many of these commands will give us the same information as did the table anyway, so it is often more convenient to skip the table and jump straight into the code. That being said, it is always valuable to look at your raw data, something which will be explored in more detail in later posts.\nAnyway, lets say we want to explicitly examine the structure of the dataset. By that I mean we want to examine the type of data each variable captures. To do this, we can use the str() command.\n\n#Examine a dataset's structure\nstr(iris_data)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nThe output of this command is quite informative. We see the total number of observations (flowers) and variables (characteristics). But we also see the structure of each variable. We can confirm now that R interprets four of the five variables as numeric, meaning it treats them as numbers. Interestingly, R does not distinguish between the type of numeric data these variables are. Although they all appear to be ratio variables, we cannot be sure without digging a bit deeper. Fortunately, we do not need to dig deeper, as in practice all that matters in nine-cases-out-of-ten is whether these data are numbers or not. In other words, in practice the distinction between ratio and interval seldom comes into play beyond the extent to which we use theory to interpret their meaning - at least not at the level of Beginners statistics.\nMeanwhile, the fifth variable in our dataset is classified as a Factor variable. This factor variable has three levels, described through words (although in the output we can only see two) and each level is associated with a specific value. You cannot really infer this directly from the output, but take it from me as being true. This variable thus appears to contain two types of data: words and numbers. What gives?\n\n\nTRUE or FALSE: The variable Species is categorical. TRUEFALSE\nTRUE or FALSE: The variable Species is interval. TRUEFALSE\nWhich of the following most accurately describes the structure of the variable Species: intervalrationominalordinal\n\n\nWell, this is something of a trick. Levels in factor variables are better thought of as labels, labels that are discrete in nature. Thus, the label setosa, for example, is really just a placeholder to give more intuitive meaning to the actual value within the cell, which is 1. This is an interesting case where the distinction between discrete and continuous variables becomes important. Factors in R are technically stored as integers under the hood, with each level corresponding to a number. You can check this by converting a factor back into numeric form: what once looked like setosa, versicolor, and virginica will suddenly appear as 1, 2, and 3.\nThe crucial point is that even though factors are stored as numbers, those numbers do not carry any quantitative meaning. Adding 1 and 2 does not give you a “hybrid” category between setosa and versicolor, and calculating an average of factor levels is meaningless. The numeric coding is only there for efficiency and to allow R to keep track of categories in a consistent way.\nThis distinction also helps us see why categorical data are considered discrete. With continuous variables such as Sepal.Length, we could in principle measure infinitely fine differences — 5.1 cm, 5.11 cm, 5.111 cm, and so on. But with a factor, there is no continuum between setosa and virginica. The categories are separate bins, and an observation must fall entirely into one of them.\n\n\n\n\n\n\nNote\n\n\n\nIndividual variables can only host one data type (i.e., nominal, ordinal, interval, or ratio). In cases where there are, for some reason, multiple different types of data (e.g., numbers and words) contained within the same column, R will revert to classifying the column according to the least restrictive classification.\nFor example, if one of your variables contains a mixture of words and numbers (e.g., zero, 1, 2, 3 …), then R will classify everything in that column as a word. Or, more precisely, as a string variable. String variables represent textual data (i.e., words) and form a separate class of data in R. What makes them interesting is that each unique string value is often mapped internally to a number, while the original string is retained as a label. This is exactly how factor variables work: for instance, the Species variable in the iris dataset is a factor. Internally, setosa, versicolor, and virginica are stored as the integers 1, 2, and 3, but R keeps the original string labels so that outputs remain readable. This mapping allows R to efficiently store categorical data and perform statistical operations that depend on levels rather than the textual content itself.\nAnother important special type is date-time data. Variables that represent dates or times, such as 2025-09-03 or 12:30:00, are stored in classes like Date or POSIXct in R. These are technically numeric under the hood (representing days or seconds since a reference point), but they behave differently because R provides specialized functions for comparison, formatting, and arithmetic with dates and times. For example, you can calculate the difference between two dates, extract the month or weekday, or filter data by date ranges. This type of data are not so important for beginners but can become quite important if you wish to work with time-series or longitudinal data, for example.\n\n\nWhen thinking about the distinction between nominal and ordinal data in R, things become a bit more tricky as you can make this distinction in R by creating “unordered” and “ordered” factor variables, but this distinction usually only matters in more advanced contexts. As a result, they will be covered in a seperate post. What is important for now is that nominal and ordinal data are both categorical. From an R perspective, this means they should take a factor form. But nominal data can also be represented using string. But, as explained above, it is difficult to do any statistics with this type of data. Meanwhile, ordinal data can be represented using numeric, but this is theoretically problematic and makes the interpretation of any statistical analysis difficult, something which will also be covered in later posts. Bottom line: categorical data are factor data.\n\n\nLooking at your data: A Primer\nWhen working with categorical variables, it is often convenient to look at how well each of the categories are represented in your data. A simple way to do this is through the table() command.\n\n#Look at categories more closely\ntable(iris_data$Species)\n\n\n    setosa versicolor  virginica \n        50         50         50 \n\n\nThe $ operator in R is a convenient way to access individual columns (variables) from a dataset. For instance, if we have the iris dataset loaded, iris$Sepal.Length returns the column (sometimes called “vector”) of all sepal lengths. Using $ is often more readable and faster than alternative methods, like indexing with iris[, \"Sepal.Length\"], and it allows you to chain operations using packages like tidyverse. One important thing to remember is that $ only works with column names that are valid R identifiers — if a column name has spaces or unusual characters, you’ll need to use a different indexing method. Using $ in combination with functions like table() makes it simple to explore and summarize individual variables (Species) in a dataset (iris_data).\nHere we see that we have an equal number of observations for each species of flower. This may or may not be meaningful, depending on your context. Where things can become interesting is when, for example, you have categories which are represented by hardly any observations. In more advanced statistical analyses, this can raise questions around, for example, whether such categories should be aggregated and combined. Usually, this type of decision is inspired by both empirics and theory. Theoretically, it becomes a question of what these categories mean and the extent to which you are actually interested in them. Empirically, it is often difficult to work with categories which only report a small number of observations.\n\n\n\n\n\n\nNote\n\n\n\nTo take another example, when studying property prices, it is often important to consider the type of building people are living in. It seems obvious that houses, boats, and apartments are probably priced differently, so this makes sense. But there are potentially dozens of way to classify a building, depending on whether you are considering architectural, land-use, and/or household differences, for example.\nMost people are not explicitly interested in all the specific categories you can make, but rather just a handful (e.g., standalone houses and apartments). In such cases, it makes sense to think about aggregating many highly detailed categories into broader groups because, theoretically, these broader groups better capture what you are interested in theoretically, while these highly disaggregated categories offer little-to-no value to your empirical work.\nYou see similar problems emerge in countless other contexts. Studies of race/ethnicity, age, and gender are three examples which come directly to mind."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching and Related Materials",
    "section": "",
    "text": "University of Groningen\n\n\n\nEconomic Geography (MSc)\n\nThis course (GEMEGTT) examines how global trends—such as technological change, climate change, and demographic transitions—affect regions differently, leading to varying economic, social, and environmental outcomes. It focuses on understanding the causes of regional socio-economic inequalities and explores how governments and other actors attempt to address them through policy at different spatial scales.\nThe course has two main aims. First, it introduces students to key theoretical and empirical debates in Economic Geography. Second, it applies these insights to real-world practices in local and regional development. Topics include regional labour markets, entrepreneurship, globalization, and spatial policy tools such as city branding, business incentives, and land-use planning.\nStudents explore both analytical concepts and empirical approaches, with a focus on the interaction between people, firms, policy, and place. The course highlights the role of institutions and governance structures in shaping regional outcomes, and discusses how development paths differ across city regions, intermediate areas, and rural zones.\nA key feature of the course is its applied orientation. Students learn how academic insights relate to practical policy decisions, preparing them for careers in regional development. This includes a study trip to Brussels to engage with institutions involved in EU regional policy.\nBy the end of the course, students are able to analyse complex geographical patterns, understand the relationship between economic and spatial processes, and assess policy responses to regional challenges. They also gain awareness of relevant labour market opportunities and the professional context of economic geographers.\n\n\nRecommended Readings\n\n\nLocal and Regional Development, by Andy Pike, Andrés Rodríguez-Pose, and John Tomaney\nAn Introduction To Geographical and Urban Economics: A Spiky World, by Steven Brakman, Harry Garretsen, and Charles van Marrewijk\nThe New Geography of Jobs, by Enrico Moretti\nTriumph of The City: How Urban Spaces Make Us Human, by Edward Glaeser\nThe Death and Life of Great American Cities, by Jane Jacobs\nOrder Without Design: How Markets Shape Cities, by Alain Bertaud\nGuns, Germs, and Steel: The Fates of Human Societies, by Jared Diamond\nThe Wealth of Cities and The Poverty of Nations, by Christof Parnreiter\nThe Great Convergence: Information Technology and The New Globalisation, by Richard Baldwin\n\n\n\n\n\nStatistics and Quantitative Research Methods (BSc)\n\nThis covers a trilogy of Introduction, Intermediate, and Applied Statistics courses which I currently teach at the University of Groningen. The studens are bachelor students studying Geography, Planning, and Demography. Relevant materials can be made available upon request.\n\nIntroductory Statistics (GESTAT1)\n\nStatistics 1 (GESTAT1) introduces students to a range of descriptive and inferential statistical techniques. Key topics include levels of measurement, spatial and non-spatial sampling, data presentation using tables and figures, and measures of central tendency and dispersion. The course also covers the central limit theorem, z-scores, z-tests, t-tests, non-parametric alternatives, and the binomial test. In addition, it addresses principles of research data management.\nStudents learn to select suitable statistical methods based on variable type and research design, and to apply these techniques using software such as R or a calculator. The course also focuses on interpreting statistical outcomes and reporting on methods, data, and results in a clear and structured manner.\nThroughout the course, attention is given to data quality, measurement accuracy, and validity. Students are expected to assess these factors carefully and apply appropriate techniques for analysis. Practical use of R supports this process, helping students to work with real data in a structured and transparent way.\n\n\nRecommended Readings\n\n\nStatistics: A Very Short Introduction by David J. Hand\nStatistics Without Tears: An Introduction For Non-Mathematicians, by Derek Rowntree\nA Field Guide To Lies and Statistics: A Neuroscientist on How to Make Sense of A Complex World, by Daniel J. Levitin\nThe Uncounted, by Alex Cobham\nElementary Statistics For Geographers by James Burt, Gerald Barber, and David Rigby\n\n\n\nIntermediate Statistics (GESTAT2)\n\nStatistics 2 (GESTAT2) focuses on multivariate statistical techniques and builds on the foundation established in Statistics 1. The course covers key methods including chi-square tests, measures of association, correlation, simple and multiple linear regression, interaction and mediation effects, as well as binary, ordinal, and multinomial logistic regression. The course places emphasis on selecting appropriate uni- and multivariate methods based on the characteristics of variables and the design of the research.\nStudents learn to select suitable statistical methods based on variable type and research design, and to apply these techniques using software such as R or a calculator. The course also focuses on interpreting statistical outcomes and reporting on methods, data, and results in a clear and structured manner.\nThroughout the course, attention is given to data quality, measurement accuracy, and validity. Students are expected to assess these factors carefully and apply appropriate techniques for analysis. Practical use of R supports this process, helping students to work with real data in a structured and transparent way.\n\n\nRecommended Readings\n\n\nMastering ’Metrics: The Path From Cause To Effect, by Joshua D. Angrist and Jörn-Steffan Pischke\nApplied Statistics Using STATA (and R): A Guide For The Social Sciences, by Mehmet Mehmetoglu and Tor Georg Jakobson\nElementary Statistics For Geographers by James Burt, Gerald Barber, and David Rigby\n\n\n\nQuantitative Research Methods (GEASRM)\n\nThis course introduces students to the full process of conducting quantitative research in the field of Spatial Sciences. It covers all stages of the research process, from formulating research questions to data collection, analysis, and presentation of results. Students learn to design and carry out a quantitative survey aligned with a clearly defined research question, using appropriate methods and techniques.\nThe course includes key topics such as data management, research ethics, and scientific integrity, with attention to responsible use of data and tools such as generative AI. Students apply quantitative methods to analyse and interpret data, and are trained to communicate findings effectively using numerical evidence.\nComputer software is used to support data analysis, with a focus on practical application within Spatial Sciences. In addition to individual work, students collaborate in teams to conduct research, with an emphasis on academic standards and constructive teamwork. The course also encourages critical reflection on the research process, data quality, and the validity of conclusions.\n\n\nRecommended Readings\n\n\nMastering ’Metrics: The Path From Cause To Effect, by Joshua D. Angrist and Jörn-Steffan Pischke\nMostly Harmless Econometrics: An Empiricist’s Companion, by Joshua D. Angrist and Jörn-Steffan Pischke\nApplied Statistics Using STATA (and R): A Guide For The Social Sciences, by Mehmet Mehmetoglu and Tor Georg Jakobson\nCausal Inference: The Mixtape, by Scott Cunningham\nThe Book of Why: The New Science of Cause and Effect, by Judea Pearl\nThe Effect: An Introduction To Research Design and Causality, by Nick Huntington-Klein\n\n\n\n\nUniversity College Cork\n\n\n\nStatistics and Econometrics (BSc)\n\nThis covers a series of courses I taught at University College Cork. The content relates to undergraduate statistics and econometrics, and was taught to students studying economics, finance, and commerce. Relevant materials can be made available upon request.\n\nIntroductory Statistics (BSc)\n\nThese courses start with introductions to descriptive statistics, inferential statistics, and probability. Much of this learning focuses on how these subjects relate to business practices. Students are introduced to elementary calculus, the mathematics of finance, differentiation, matrix algebra, alongside exponential, logarithmic, and polynomial functions. Students use this knowledge to calculate elasticities, solve for equilibria, and work with series and annuities.\nFrom there, a major emphasis is placed on the practical application of statistics to business contexts. Teaching centres around the practicalities of working with data. Students are exposed to key software used for analysing economic and financial data, such as Excel and STATA. They are also tasked with collecting, analysing, interpreting, and presenting economic data in written reports. Supplementing this are introductions to the numerous types of economic and financial data available, and the outlets from which these data can be acquired.\nIn University College Cork, I have been involved in the following courses which fit this description in different capacities:\nEC1111 (Economic Data Collection) EC1200 (Quantitative Techniques For Economics 1) EC1209 (Understanding and Interpreting Data) MA1100 (Introductory Mathematics For Business)\n\n\nRecommended Readings\n\n\nStatistics: A Very Short Introduction by David J. Hand\nStatistics Without Tears: An Introduction For Non-Mathematicians, by Derek Rowntree\nA Field Guide To Lies and Statistics: A Neuroscientist on How to Make Sense of A Complex World, by Daniel J. Levitin\nThe Uncounted, by Alex Cobham\nWhy Most Things Fail, by Paul Ormerod\nElementary Statistics For Geographers by James Burt, Gerald Barber, and David Rigby\n\n\n\nIntermediate Statistics and Introductory Econometrics (BSc)\n\nThese courses build upon foundational statistics and probability knowledge by equipping students with the skills to critically evaluate economic research, data, methods, and findings. This is supplemented by introductions to hypothesis testing and statistical inference using cross-sectional and time-series methods.\nFrom there, students are exposed to the relationships between economic theory and practice, and how different modelling techniques can be applied to different scenarios within a business context. In particular, simple linear regression, multiple linear regression, functional forms, and variable transformations are introduced while assessment emphasizes the interpretation of findings and implications of using different empirical techniques.\nIn University College Cork, I have been involved in the following courses which fit this description in different capacities:\nEC1210 (Skills For Analysing Economic Data) EC2015 (Research in Economics) EC2116 (Introduction To Statistical Economic Analysis) EC2206 (Business Econometrics and Forecasting) EC4215 (Business Econometrics 1) Excel and STATA material\n\n\nRecommended Readings\n\n\nMastering ’Metrics: The Path From Cause To Effect, by Joshua D. Angrist and Jörn-Steffan Pischke\nMostly Harmless Econometrics: An Empiricist’s Companion, by Joshua D. Angrist and Jörn-Steffan Pischke\nEconometrics By Example, by Damodar Gujarati\nIntroductory Econometrics: A Modern Approach, by Jeffrey Wooldridge\nRegression Models For Categorical Dependent Variables Using STATA, by Scott Long and Jeremy Freese\nApplied Statistics Using STATA (and R): A Guide For The Social Sciences, by Mehmet Mehmetoglu and Tor Georg Jakobson\nCausal Inference: The Mixtape, by Scott Cunningham\nProbability and Statistics For Economists, by Bruce Hansen\nThe Effect: An Introduction To Research Design and Causality, by Nick Huntington-Klein\nThe Book of Why: The New Science of Cause and Effect, by Judea Pearl\nFooled By Randomness: The Hidden Role of Chance in Life and in The Markets, by Nassim Nicholas Taleb\nThe Black Swan: The Impact of The Highly Improbable, by Nassim Nicholas Taleb\nSkin In The Game: Hidden Asymmetries in Daily Life, by Nassim Nicholas Taleb\nAntifragile: Things That Gain From Disorder, by Nassim Nicholas Taleb\n\n\nAlongside these reading materials, I have also produced Introductory Guides and Practical Materials for STATA and Excel which can be made available upon request. Topics covered include:\n\nDescriptive Statistics and Data Visualisation (STATA, Excel, and R)\nMoving Averages and Forecasting (Excel)\nSimple and Multiple Linear Regression (STATA and R)\nVariable Transformations and Functional Forms (STATA and R)\nBinary, Multinomial, and Ordinal Logistic Regression (R)\n\n\n\n\nMacroeconomics (BSc)\n\nThis covers a series of courses I taught at University College Cork. The content relates to undergraduate macroeconomics, and was taught to students studying economics, finance, and commerce. Relevant materials can be made available upon request.\n\nIntroductory Macroeconomics\n\nThese courses start with the study of the economy as a whole, focusing on aggregate measures such as national income, unemployment, inflation, and economic growth. Students learn to work with theories of supply and demand, fiscal and monetary policy, international trade, and exchange rates, while also covering national income accounting, the measurement of economic activity, and the circular flow of income.\nThe role of government in influencing economic outcomes is also explored. Topics such as fiscal policy, monetary policy, and the role of central banks are discussed, along with the challenges and trade-offs involved in implementing macroeconomic policies. This provides students with opportunities to apply theoretical concepts to analyze current economic events and policy debates, enhancing their critical thinking and analytical skills.\nIn University College Cork, I have been involved in the following courses which fit this description in different capacities:\nEC1116 (Introductory Macroeconomics) EC1208 (Principles of the Macro Economy) EC1503 (Economic Analysis for Food Business) EC1122 (Markets, Governments and the Economics of Social Issues)\n\n\nRecommended Readings\n\n\nThe Great Economists: How Their Ideas Can Help Us Today, by Linda Yueh\nHow To Think Like An Economist, by Robbie Mochrie.\nCan’t We Just Print More Money? Economics in Ten Simple Questions, by Jack Meaning\nThe Great Crashes: Lessons From Global Meltdowns and How To Prevent Them, by Linda Yueh\nGlobalization and Its Discontents, by Joseph Stiglitz\nDoughnut Economics, by Kate Raworth\nGood Economics For Hard Times: Better Answers To Our Biggest Problems, by Abhijit V. Banergee and Esther Duflo\nPoor Economics, by Abhijit V. Banergee\nNature, Culture, and Inequality, by Thomas Picketty\nA Brief History of Equality, by Thomas Picketty\nWhy Nations Fail: The Origins of Power, Prosperity, and Poverty, by Daron Acemoglu and James A. Robinson\n\n\n\nIntermediate Macroeconomics\n\nThese courses build upon the work done in introductory modules by focusing on specific themes and issues within macroeconomics. Themes and topics studied include human capital, inequality, money and monetary policy, alongside national and international economic variables which affect business performance.\nSome of these courses then pivot to focus on social issues and policy, thereby requiring detailed analysis of issues like healthcare, pensions, and the welfare state, while some of these courses focus on financial markets. An overarching theme in these courses, however, is the role of governmental intervention in the economy alongside how Ireland is situated in the global macroeconomic environment. This provides students with opportunities to apply theoretical concepts to analyze current economic events and policy debates, enhancing their critical thinking and analytical skills.\nIn University College Cork, I have been involved in the following courses which fit this description in different capacities:\nEC2010 (The Changing Economy: Money and Monetary Policy) EC2112 (Macroeconomics: Irish and International Business Cycles) EC2151 (Economics of Social Policy 1) EC2220 (Microeconomics and Macroeconomic Outcomes) EC2214 (The Macroeconomic Environment in a Global Context) EC3151 (Economics of Social Policy 2)\n\n\nRecommended Readings\n\n\nMacroeconomics, by Daron Acemoglu, David Laibson, and John A. List\nEconomics For The Common Good, by Jean Tirole\nRisky Business: Why Insurance Markets Fail and What To Do About It, by Amy Finkelstein, Liran Einav, and Raymond Fisman\nThe Entrepreneurial State: Debunking Public VS Private Sector Myths, by Mariana Mazzucato\nThe Price of Inequality, by Joseph Stiglitz\nWhy Most Things Fail, by Paul Ormerod\nSlouching Towards Utopia: An Economic History of the Twentieth Century, by J. Bradford DeLong\nEconomyths: How The Science of Complex System is Transforming Economic Thought, by David Orrell\nA Random Walk Down Wall Street: The Time-Tested Strategy For Successful Investing, by Burton G. Malkiel\nThe Intelligent Investor, by Benjamin Graham\n\n\n\n\n\nMicroeconomics (BSc)\n\nThis covers a series of courses I taught at University College Cork. The content relates to undergraduate microeconomics, and was taught to students studying economics, finance, and commerce. Relevant materials can be made available upon request.\n\nIntroductory Microeconomics\n\nThese courses start by introducing fundamental economic principles at the individual level, focusing on supply, demand, market equilibrium, opportunity costs, and elasticity. The material then evolves to emphasize consumer and producer behaviour under different conditions and within different market contexts.\nThese courses then pivot to consider the role of firms and governments in the microeconomic environment. Of particular interest here is the efficient allocation of resources and leads students to a more nuanced understanding of how policies such as taxation, subsidies, and regulations affect market outcomes.\nIn University College Cork, I have been involved in the following courses which fit this description in different capacities:\nEC1117 (Markets and Governments: An Introduction to Economics) EC1213 (Microeconomic Reasoning and Practice) EC1202 (Economic Reasoning For Business) EC1212 (Economics of Business 1) EC1500 (Ethics and Economic Decision Making in Food Business)\n\n\nRecommended Readings\n\n\nThe Great Economists: How Their Ideas Can Help Us Today, by Linda Yueh.\nHow To Think Like An Economist, by Robbie Mochrie.\nFreakonomics: A Rogue Economist Explores The Hidden Side of Everything, by Steven D. Levitt and Stephen J. Dubner.\nSuperFreakonomics: Global Cooling, Patriotic Prostitutes, and Why Suicide Bombers Should Buy Life Insurance, by Steven D. Levitt and Stephen J. Dubner.\nThink Like A Freak, by Steven D. Levitt and Stephen J. Dubner.\nThe Undercover Economist, by Tim Harford.\nHow To Teach Economics To Your Dog: A Quirky Introduction, by Rebecca Campbell and Anthony McGowan.\nEdible Economics: A Hungry Economist Explains The World, by Ha-Joon Chang.\n\n\n\nIntermediate Microeconomics\n\nThese courses build upon introductory material by focusing on the interactions between producers and consumers within different types of markets. In particular, entrepreneurship, the competitive process and technological change, the determination of the relative prices of goods, and factors of production under various types of market structure are all covered.\nThese courses then pivot in three distinct directions to focus on the government, the firm, and labour market. We consider the role of government in achieving social goals and public policy objectives. Students examine how government actions impact resource allocation, income distribution, and overall welfare. For the firm, we focus on budgets, risk and uncertainty, revenue functions, alongside production and cost analysis. While in labour markets, we emphasize the factors driving labour supply and demand, the acquisition of training and skills, technology and decision making, alongside government intervention in such markets, such as through minimum wages and taxation.\nIn University College Cork, I have been involved in the following courses which fit this description in different capacities:\nEC1121 (Markets, Governments, and The Economics of Social Issues) EC2200 (Economics of Managerial Decision Making) EC3127 (Economics of The Labour Market) EC4211 (Economics of The Labour Market)\n\n\nRecommended Readings\n\n\nThe Great Economists: How Their Ideas Can Help Us Today, by Linda Yueh.\nHow To Think Like An Economist, by Robbie Mochrie.\nFreakonomics: A Rogue Economist Explores The Hidden Side of Everything, by Steven D. Levitt and Stephen J. Dubner.\nSuperFreakonomics: Global Cooling, Patriotic Prostitutes, and Why Suicide Bombers Should Buy Life Insurance, by Steven D. Levitt and Stephen J. Dubner.\nThink Like A Freak, by Steven D. Levitt and Stephen J. Dubner.\nThe Undercover Economist, by Tim Harford.\nHow To Teach Economics To Your Dog: A Quirky Introduction, by Rebecca Campbell and Anthony McGowan.\nEdible Economics: A Hungry Economist Explains The World, by Ha-Joon Chang.\nNudge: Improving Decisions About Health, Wealth, and Happiness, by Richard H. Thaler and Cass R. Sunstein.\nMisbehaving: The Making of Behavioral Economics, by Richard H. Thaler.\nSmall Is Beautiful: A Study of Economics As If People Mattered, by Ernst F. Schumacher\nMicroeconomics by N. Gregory Mankiw and Mark P. Taylor.\nMicroeconomic Foundations I: Choice and Competitive Markets by David M. Kreps.\nMicroeconomic Foundations II: Imperfect Competition, Information, and Strategic Interaction by David M. Kreps.\nChicago Price Theory by Sonia Jaffe, Robert Minton, Casey B. Mulligan, and Kevin M. Murphy.\nMicroeconomic Theory by Andreu Mas-Collel, Michael Whinston, and Jerry Green.\n\n\n\n\nResearch Supervision (BSc, MSc, and PhD)\n\n\n\nSome Background\n\nI thoroughly enjoy supervising research projects, especially those at the MSc and PhD level. The topics I am interested in broadly fall under the umbrella subjects of Urban Studies, Urban Economics, Regional Science, Economic Geography, and Transportation. If the core focus of a given project falls within the remit of any of these subjects, then chances are that I will be interested in supervising it. One thing that I am passionate about is writing, and ensuring that students produce a high-quality written product which is among the top priorities for me when I supervise independent research projects.\nTo help students along in this journey, I have created a writing guide which provides a broad scaffold students can use when learning how to write effectively. You can access this guide here.\nIf you are a student and especially interested in working on a specific topic under my direct supervision, please email me at c [dot] odriscoll [at] rug [dot] nl\n\n\n\nBSc Research Project Supervision\n\nAt the University of Groningen, I am heavily involved in the supervision of BSc theses. Usually, I take two groups of students: one per semester. When I supervise these groups on my own, my theme of choice is Cities.\nI’m drawn to the subject of cities because they illuminate how human potential is concentrated, expressed, and contested in space. For much of history, cities have been foundational to economic growth, innovation, and social change. Their dense networks of people, ideas, and resources create agglomeration effects that not only boost productivity but also shape everyday life and long-term regional prosperity. Cities are not just economic engines—they are complex living systems where geography, economics, planning, and sociology converge, offering a lens into the structures and dynamics that define the modern world.\nWhat fascinates me is this dual nature: cities are sites of opportunity and dynamism, but also of tension and inequality. The very features that make urban areas productive—density, connectivity, diversity—also generate challenges like housing shortages, congestion, environmental degradation, and social fragmentation. As urbanisation accelerates and the spatial distribution of economic activity becomes increasingly uneven, cities expose and amplify regional disparities, while also being on the front lines of global risks such as climate change, public health crises, and economic shocks.\nStudying cities, for me, is a way to understand how spatial processes shape economic and social outcomes, how individual and collective decisions are mediated by place, and how policy can engage with these dynamics to foster more equitable and resilient futures. Cities aren’t just where things happen—they are how things happen.\nWithin this group, I place no hard restrictions on the type of questions students can tackle, so long as those questions are intricately linked to cities. Given my background, I like to stress that the broad subjects students should focus on within this theme include Urban Studies, Urban Economics, Economic Geography, Regional Science, and Transportation\nSometimes, I share group supervision duties. In this context, the theme can change to suit the expertise of the person I am working with. I have listed some of these themes below:\n\nTo Be Added.\nTo Be Added.\n\n\n\n\nMSc Thesis Supervision\n\nAt the University of Groningen, I am heavily involved in the supervision of MSc thesis students in the Economic Geography and Real Estate Studies programme. Much like my philosophy for supervising BSc students, I tend to cast a wide net topic-wise, and encourage any quantitatively-oriented student to reach out if they have a cool idea they would like to explore which falls under the broad remit of Urban Studies, Urban Economics, Economic Geography, Regional Science, and Transportation.\n\n\n\nPhD Thesis Supervision\n\nI have no formal experience supervising PhD students, yet. Watch this space as in the coming months this will change."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Publications and Talks",
    "section": "",
    "text": "Conor O’Driscoll. 2025. “Commuting in Flux: The Roles of Place and Personal Circumstance In Shaping Behavioural Plasticity”.\n\nPowerPoint Slides: ERSA 2025, Athens, Greece\n\nConor O’Driscoll and Milad Abbasiharofteh. 2025. “Roots and Routes: Residential Relocation and Relatedness”.\nConor O’Driscoll and Federica Rossi. 2025. “Residential Relocation Decisions and Destinations: The Role of Working From Home”.\n\nPowerPoint Slides: ERSA 2025, Athens, Greece\n\nConor O’Driscoll and Ana Maria Silva. 2025. “Working From Home and Labour Market Outcomes: The Case of Earnings and Hours Worked”.\nConor O’Driscoll and Luise Koeppen. 2025. “Subnational Institutional Corruption and Political Discontent”.\nConor O’Driscoll. 2025. “Navigating Change: Residential Relocation, Travel Behaviours, and Built Environments”. Major Revisions in Papers in Regional Science.\n\nPowerPoint Slides: RSAI BIS 2024, Bristol, UK"
  },
  {
    "objectID": "research.html#select-ongoing-projects",
    "href": "research.html#select-ongoing-projects",
    "title": "Publications and Talks",
    "section": "",
    "text": "Conor O’Driscoll. 2025. “Commuting in Flux: The Roles of Place and Personal Circumstance In Shaping Behavioural Plasticity”.\n\nPowerPoint Slides: ERSA 2025, Athens, Greece\n\nConor O’Driscoll and Milad Abbasiharofteh. 2025. “Roots and Routes: Residential Relocation and Relatedness”.\nConor O’Driscoll and Federica Rossi. 2025. “Residential Relocation Decisions and Destinations: The Role of Working From Home”.\n\nPowerPoint Slides: ERSA 2025, Athens, Greece\n\nConor O’Driscoll and Ana Maria Silva. 2025. “Working From Home and Labour Market Outcomes: The Case of Earnings and Hours Worked”.\nConor O’Driscoll and Luise Koeppen. 2025. “Subnational Institutional Corruption and Political Discontent”.\nConor O’Driscoll. 2025. “Navigating Change: Residential Relocation, Travel Behaviours, and Built Environments”. Major Revisions in Papers in Regional Science.\n\nPowerPoint Slides: RSAI BIS 2024, Bristol, UK"
  },
  {
    "objectID": "research.html#published-research-papers",
    "href": "research.html#published-research-papers",
    "title": "Publications and Talks",
    "section": "Published Research Papers",
    "text": "Published Research Papers\n\nConor O’Driscoll; Frank Crowley; Justin Doran; Nóirín McCarthy; Josh O’Driscoll. 2025. “Travel Behaviours and Built Environments on School-Runs”. Regional Science Policy and Practice, 17(1), pp.1-14. doi: 10.1016/j.rspp.2024.100153.\nKevin Credit and Conor O’Driscoll. 2024. “Assessing Modal Tradeoffs and Associated Built Environment Characteristics Using a Cost-Distance Framework”. Journal of Transport Geography, 117, pp.1-19. doi: 10.1016/j.jtrangeo.2024.103870.\nConor O’Driscoll; Frank Crowley; Justin Doran; Nóirín McCarthy. 2024. “The relationships between socio-demographics, residential environments, travel considerations, and commute mode choice in Ireland”. Regional Studies, 58(3), pp.1-18. doi: 10.1080/00343404.2023.2199779.\nConor O’Driscoll; Frank Crowley; Justin Doran; Nóirín McCarthy. 2023. “Land-Use Mix in Ireland: Implications for Sustainable Development”. Journal of Maps, 19(1), pp.1-7. doi: 10.1080/17445647.2023.2214165.\nConor O’Driscoll; Frank Crowley; Justin Doran; Nóirín McCarthy. 2023. “Land-Use Mixing in Irish Cities: Implications for Sustainable Development”. Land Use Policy, 128(5), pp.1-7. doi: 10.1016/j.landusepol.2023.106615.\nConor O’Driscoll; Frank Crowley; Justin Doran; Nóirín McCarthy. 2022. “Retail Sprawl and CO2 Emissions: Retail Centres in Irish Cities”. Journal of Transport Geography, 102(6), pp.1-12. doi: 10.1016/j.jtrangeo.2022.103376."
  },
  {
    "objectID": "research.html#public-outreach-research-dissemination",
    "href": "research.html#public-outreach-research-dissemination",
    "title": "Publications and Talks",
    "section": "Public Outreach / Research Dissemination",
    "text": "Public Outreach / Research Dissemination\n\nConor O’Driscoll. 2024. “The Cost of Morning Commutes: The Case of Kildare”. Kildare FM.\nConor O’Driscoll and Kevin Credit. 2024. “Here’s the real cost of your morning rush hour commute in Dublin”. RTE Brainstorm.\nConor O’Driscoll; Frank Crowley; Justin Doran; Nóirín McCarthy. 2023. “How the relationship between socio- demographics, residential environments and travel influence commuter choices”. Regional Studies Blog.\nConor O’Driscoll. 2023. “Retail Centre Locations in Cork: The Case of Carrigtwohill”. RTE Prime Time.\nConor O’Driscoll; Frank Crowley; Justin Doran; Nóirín McCarthy. 2022. “Planning For Sustainability: Future Retail Centre Locations”. The Boolean, 6(8), pp.27-32.\nConor O’Driscoll; Frank Crowley; Justin Doran; Nóirín McCarthy. 2022. “The Links Between Where We Live and How We Commute”. RTE Brainstorm.\nJane Bourke; Josh O’Driscoll; Conor O’Driscoll. 2022. “Does fear of failure hamper Irish business innovation?”. RTE Brainstorm.\nConor O’Driscoll. 2021. “Why out of town retail parks don’t make sense in a climate crisis”. RTE Brainstorm.\nConor O’Driscoll. 2021. “Shopping Malls, GHG Emissions and The Role of Policymakers in”Green” Transportation Infrastructure in Ireland”. Regional Studies Association: Student Summer Series."
  },
  {
    "objectID": "research.html#talks",
    "href": "research.html#talks",
    "title": "Publications and Talks",
    "section": "Talks",
    "text": "Talks\n\nConor O’Driscoll. 2025. “Mobility and Economic Geography”.\n\nAn invited early-career keynote speech to be delivered at the Regional Science Association International: British and Irish Section. The conference was held in Cork, Ireland.\n\nConor O’Driscoll. 2025. “Stuck in the Mud? Geographical Immobility Across The UK”.\n\nPresentation of an ongoing research project at the Regional Science Association International: British and Irish Section. The conference was held in Cork, Ireland.\n\nConor O’Driscoll. 2024. Navigating Change: The PhD Journey and Life After.\n\nAn invited workshop by colleagues at the Spatial and Regional Economic Research Centre, University College Cork. Topics covered in the workshop include: Navigating the PhD Journey, The Research Process, Publishing Research, and The Academic Job Market.\n\nConor O’Driscoll. 2024. Land-Use Patterns and Commuting in Cork.\n\nThis invited workshop, hosted by the Regional Studies Association (Irish Section), covered the topic of pressing transportation issues in Ireland. At this workshop, I presented ongoing research about commuting flows in the Cork Metropolitan Area, while other notable speakers included Dr. Kevin Credit of Maynooth University and Prof. Hannah Daly of University College Cork.\n\nConor O’Driscoll. 2024. Writing and Research in University.\n\nThis invited talk was hosted for undergraduate students about to undertake the first research project of their academic journeys. The purpose of this talk was to introduce them to writing conventions, techniques, and illustrate expectations surrounding what is “expected” in the writings of a university student."
  },
  {
    "objectID": "posts/ersa_2025.html",
    "href": "posts/ersa_2025.html",
    "title": "ERSA 2025: Some Reflections",
    "section": "",
    "text": "Over the past week i had the pleasure of attending the 65th annual conference of the regional science association.\nOverall, the conference was a major success for me. I got to meet some really interesting people, especially: Steven Bond-Smith, Inessa Tregubova, Tuomas Väisänen, Liv Osland, and Lukas Makovsky. But I also got to catch up with some old friends who I seldom see outside of such events.\nThe conference venue was interesting: a run-down university just outside the city centre of Athens (a truly beautiful city, I must stress). There were many many (over 900!) presenters and the sessions were long, so the days were quite intense. But the quality of presentations was, on balance, reasonably high, which facilitated many nice and stimulating discussions on pressing topics for spatial and social scientists. The food at the conference was amazing, culminating in a fabulous dinner in an events centre type-of-place that overlooked the whole city and harbour. It was breathtaking.\nAttending conferences is a super nice opportunity for academics to share and discuss the research they are currently undertaking. In doing so, we usually receive quite helpful feedback on making our projects better, but also get to meet likeminded people who like to think about similar problems. This can lead to potentially exciting collaborations on different research projects, but it more generally allows you to expand your professional network and keep pace with the rapidly developing knowledge base in your respective field.\nAt ERSA, I presented two research papers I am currently working on. The first concerns the relationship between widespread working-from-home adoption and locational mobility. More specifically, Federica Rossi and I explore the relationship between working from home frequently and i) the probability that individuals relocate their residence and ii) the probability of individuals moving to specific types of places (i.e., cities, suburbs, and rural areas).\nThe discussion surrounding this paper was really stimulating, and i had the pleasure of meeting really impressive scholars who are working on tangential issues. The result? Some really nice discussions on i) how to make my current research better and more impactful, and ii) how we might collaborate together on future research projects.\nThe second project is my first solo-authored research paper. The core focus of this paper is to explore the relative importance of local built environments and major life events in shaping commuting behaviour. Theoretically, our understanding of these dynamics is somewhat limited. And I have some pretty cool data that allows me to explore this issue in great detail. So I also applied for the prestigious Epainos award - an award that recognises excellence in early career research.\nUnfortunately, I did not win the award. Inessa Tregubova won the Epainos award in 2025 for her innovative work linking GPS data, working-from-home proclivities, and spatial equilibrium models - a very very cool project. On my end, I received terrific feedback on how I might make my paper more impactful from a theoretical and empirical perspective; and I am hoping to use this feedback to inspire my application for next year’s Epainos award.\nWith a new academic year starting up again, I thought it potentially useful to reflect on my experiences, as it will be many months before I get the opportunity to attend another international conference. Some takeaway points for me are:\n\n2 hour sessions are too long. 90 minutes is just right.\nThere should not be more than 4 presenters in any session.\nSessions dedicated to Young Scientists are an amazing initiative, and it is especially nice that ERSA schedules all such sessions on their own - providing excellent exposure to early career academics.\nI need to become a bit better at i) preparing the narrative of my presentations and ii) planning my presentations so that they are ~5 minutes shorter than the allocated time (delays are inevitable).\nConferences are super fun and i am excited to go to more."
  },
  {
    "objectID": "posts/thesis_1000_words.html",
    "href": "posts/thesis_1000_words.html",
    "title": "How to write a thesis in 1000 words or less",
    "section": "",
    "text": "Take a deep breath\nWriting a thesis can feel overwhelming at first, but it really doesn’t have to be. Whether you’re working on a BSc or MSc thesis, this process should be a rewarding and even enjoyable experience. If done right, it’s one of the few moments in your studies where you get to really take the lead—asking a question you care about and seeing where it takes you.\nStart with something that interests you. This sounds obvious, but it matters. When students are genuinely curious about their topic, the whole process works better. They read more deeply, write more thoughtfully, and engage with feedback more meaningfully. You don’t need a perfect question from the outset, especially at BSc level, but you do need a spark. Something you want to understand better.\n\n\nThe bigger picture\nA good thesis is also one that’s feasible. Big, complex questions are appealing in theory, but they’re rarely manageable within the scope of a student project. The best work I’ve supervised often focuses on one method and one main idea, carried out with clarity and care. It’s better to do something simple well than to do something ambitious halfway.\nAt MSc level, more independence is expected from the start. Students are generally expected to arrive with a clear idea of their topic and to develop a more original contribution to the academic literature. That said, support is still there, and the same principles apply: clarity over complexity, structure over improvisation, and a well-reasoned approach to methods and writing.\nFor BSc students, the process is more structured. The work is often group-based, with staged assignments and regular feedback. This helps students stay on track and provides a useful framework for what, for many, is their first piece of independent research.\nRegardless of level, writing is the central skill. A thesis lives and dies on how well it’s written. This doesn’t mean flowery language or complicated phrasing, it means clear, coherent thinking on the page. Readers shouldn’t have to guess what you mean or how your argument fits together. Every section should do a job, and every paragraph should move the argument forward.\n\n\nThe section-specific details\nThe Abstract should summarise what you studied, how you studied it, and what you found: brief, factual, and to the point.\nThe Introduction needs to make the case for your research: why the topic matters, what the main question is, how you approach it, and how it contributes to the broader literature.\nThe Theoretical Framework should not just list what others have said, but explain how you’re thinking about the topic and what concepts or hypotheses guide your analysis.\nThe Data and Methods section explains what materials you used and how you analysed them. This should be clear enough that someone else could, in theory, replicate what you did independently.\nThe Results section should present your findings clearly and systematically, without overloading the reader; if you include a Discussion here, use it to interpret what those findings mean in light of your research question and theory.\nThe Conclusion ties everything together: what you found, why it matters, what the limitations were, and where the research could go next.\nFormatting, structure, and references matter too, but the real foundation is clarity—of thought, of question, of execution.\n\n\nOwnership and Expectations\nAbove all, take ownership of your work. Supervisors are there to help, but they won’t, and can’t, catch everything. The best students use feedback wisely, plan their time well, and take responsibility for shaping their project. This is your research, and the final thesis should reflect how you think, not just what you did.\nWriting a thesis isn’t just about ticking boxes: it’s about learning how to ask better questions, how to test ideas, and how to communicate your findings. It’s a challenge, yes, but also a rare opportunity. With the right focus and a bit of care, it can be a genuinely rewarding part of your academic life.\n\n\nLooking for more?\nIf you found this post useful, I suggest you check out the fully-fleshed out writing guide that I distribute to all the students under my supervision. You can find it here."
  },
  {
    "objectID": "posts/data_vis.html",
    "href": "posts/data_vis.html",
    "title": "Always visualise your data: Simpsons Paradox in action",
    "section": "",
    "text": "If you’ve ever opened a dataset and jumped straight into statistical testing, you’re not alone. It’s tempting to rush toward a result — an effect, a relationship, a difference — and get to work writing it up. But what if the “result” you find hides a deeper contradiction? What if the truth is visible in your data, but only if you look at it the right way?\nThis is where data visualization comes in. Visualization is how we make our data legible not just to others, but to ourselves. Indeed, I would make the case that you cannot fully understand what is going on in your data without some form of data visualisation as it helps us detect patterns, check assumptions, and avoid being misled.\nTo illustrate this, let’s explore one of the most famous cases where misleading aggregate data can lead to erroneous conclusions: Simpson’s Paradox, using the classic UC Berkeley admissions dataset from 1973.\n\nSetting The Scene\nImagine that you’re interested in studying gender bias in university admissions. You obtain real administrative data from UC Berkeley’s graduate programs from 1973 and start with what seems like a straightforward question:\nWere men more likely to be admitted than women?\nYou begin by exploring what type of data you have available in your dataset.\nIn this post, we will use the UCBAdmissions dataset: A well-known built-in R dataset derived from real administrative records of graduate admissions at UC Berkeley in 1973.\n\n#The relevant packages have been loaded elsewhere\n\n#Load the data\ndata(UCBAdmissions)\nucb_df &lt;- as.data.frame(UCBAdmissions)\n\n#What are the variable names?\nnames(ucb_df)\n\n[1] \"Admit\"  \"Gender\" \"Dept\"   \"Freq\"  \n\n#A broad overview of the structure these variables take\nglimpse(ucb_df)\n\nRows: 24\nColumns: 4\n$ Admit  &lt;fct&gt; Admitted, Rejected, Admitted, Rejected, Admitted, Rejected, Adm…\n$ Gender &lt;fct&gt; Male, Male, Female, Female, Male, Male, Female, Female, Male, M…\n$ Dept   &lt;fct&gt; A, A, A, A, B, B, B, B, C, C, C, C, D, D, D, D, E, E, E, E, F, …\n$ Freq   &lt;dbl&gt; 512, 313, 89, 19, 353, 207, 17, 8, 120, 205, 202, 391, 138, 279…\n\n\nOk. These commands already tells us a lot about the structure of the dataset. There are four variables (columns) and 24 unique data points (rows). More specifically, we can see that it counts the number of applications for six departments by admissions status and sex. To really ensure that you understand the structure of this dataset, try answering the following questions:\n\n\nWhat type of variable is Admit? RatioIntervalBinaryNominalOrdinal\nWhat type of variable is Gender? RatioIntervalNominalOrdinal\nWhat type of variable is Freq? RatioIntervalBinaryNominalOrdinal\nWhat type of variable is Dept? RatioIntervalBinaryNominalOrdinal\nWhat type of variable is labelled as fct? BinaryNominalOrdinalAll of these\nWhat type of variable is labelled as dbl? IntervalRatioBoth of these\n\n\n\n\nAggregate Trends: A Broad Overview\nNow that we know how our dataset is structured and broadly what the variables look like, we can revisit our main research question: Were men more likely to be admitted than women?\nHow might we begin answering such a question? A logical first step might be to summarize overall admission rates by sex:\n\n#Calculate overall admission rates by gender\noverall_admit &lt;- ucb_df %&gt;%\n  group_by(Gender, Admit) %&gt;%\n  summarise(Freq = sum(Freq)) %&gt;%\n  tidyr::pivot_wider(names_from = Admit, \n                     values_from = Freq) %&gt;%\n  mutate(Total = Admitted + Rejected,\n         AdmitRate = Admitted / Total) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'Gender'. You can override using the\n`.groups` argument.\n\n#Display the table\nprint(overall_admit)\n\n# A tibble: 2 × 5\n  Gender Admitted Rejected Total AdmitRate\n  &lt;fct&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 Male       1198     1493  2691     0.445\n2 Female      557     1278  1835     0.304\n\n\nTo ensure that you understand what this code is doing, try answering the following questions:\n\n\nTrue or False: Grouping by Gender and Admit allows us to generate seperate counts of admissions and rejections by Gender and Department. TRUEFALSE\nTrue or False: Grouping by Admit is unnecessary to generate the values used to compute AdmitRate. TRUEFALSE\nIf we wanted to make this table more professional in its presentation, which of the following packages might we use? ggplot2tidyversekableExtrastargazer\nWhich type of table does this example most closely resemble? summary statistics tablecrosstablefrequency table\n\n\nAt first glance, the results seem clear: men are more likely to be admitted to graduate school in UC Berkeley than woman.\n\n\nWhich column do we use to come to this conclusion? GenderAdmittedRejectedTotalAdmitRate\n\n\n\n\nDigging A Bit Deeper: Department-Specific Heterogeneity?\nYou might stop here and think your work is done. But this only tells us what is going on in the aggregate. Unless you are a macroeconomist, you should know better than to trust aggregate data; something interesting probably lies beneath the surface. So, with this in mind, we shall dig a bit deeper.\nLet’s start by breaking this down by department. The data includes six departments, labeled A–F. What happens when we examine admission rates within departments?\n\ndept_admit &lt;- ucb_df %&gt;%\n  group_by(Dept, Gender, Admit) %&gt;%\n  summarise(Freq = sum(Freq)) %&gt;%\n  tidyr::pivot_wider(names_from = Admit, \n                     values_from = Freq) %&gt;%\n  mutate(Total = Admitted + Rejected,\n         AdmitRate = Admitted / Total)\n\n`summarise()` has grouped output by 'Dept', 'Gender'. You can override using\nthe `.groups` argument.\n\nprint(dept_admit)\n\n# A tibble: 12 × 6\n# Groups:   Dept, Gender [12]\n   Dept  Gender Admitted Rejected Total AdmitRate\n   &lt;fct&gt; &lt;fct&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1 A     Male        512      313   825    0.621 \n 2 A     Female       89       19   108    0.824 \n 3 B     Male        353      207   560    0.630 \n 4 B     Female       17        8    25    0.68  \n 5 C     Male        120      205   325    0.369 \n 6 C     Female      202      391   593    0.341 \n 7 D     Male        138      279   417    0.331 \n 8 D     Female      131      244   375    0.349 \n 9 E     Male         53      138   191    0.277 \n10 E     Female       94      299   393    0.239 \n11 F     Male         22      351   373    0.0590\n12 F     Female       24      317   341    0.0704\n\n\n\n\nOne of the following statements best describes the core difference between dept_admit and overall_admit. Pick the most appropriate:\n\n overall_admit counts admissions and rejections by sex; dept_admit further breaks this down by sex overall_admit gives us aggregate trends while dept_admit provides more disaggregated insights dept_admit fails to account for admission status while overall_admit does\n\n\n\nNow the story flips: in most departments, women have higher admission rates than men. So how can the overall numbers suggest the opposite? This reversal is a textbook example of Simpson’s Paradox - a phenomenon where a trend appears in different groups but reverses when the groups are combined.\nIn this case, women were more likely to apply to departments with lower overall admission rates (e.g., departments C, D, E, F), while men applied more to departments with higher admission rates (departments A and B). The aggregate numbers hide this because they mix different denominators across departments.\nThis illustrates a broader lesson: data summaries without disaggregation can obscure the underlying structure of your data. And without visualization, this kind of paradox is hard to detect.\nLet’s visualize the department-level admission rates by gender.\n\nggplot(dept_admit, aes(x = Dept, \n                       y = AdmitRate, \n                       fill = Gender)) +\n  geom_col(position = \"dodge\") +\n  labs(title = \"Admission Rates by Gender and Department (UC Berkeley, 1973)\",\n       y = \"Admission Rate\", \n       x = \"Department\") +\n  scale_fill_manual(values = c(\"Male\" = \"#377eb8\", \n                               \"Female\" = \"#e41a1c\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nWhich of the following charts best describes the chart displayed above? histogrampie chartbar chartscatterplotstem-and-leaf diagram\nWhat do the letters x and y refer to, in statistical terms?\n\n X refers to an independent variable; Y refers to a dependent variable X refers to an dependent variable; Y refers to a independent variable X refers to an endogenous variable; Y refers to a exogenous variable X refers to the variable which determines the value of Y Y refers to the variable which determines the value of X\n\nWhich of the following might best describe why we have put AdmitRate as the Y variable in this chart?\n\n Because admission rates determine which department an applicant chooses Because we want to compare how likely applicants are to be admitted across departments and genders Because AdmitRate is a control variable in the admissions process Because AdmitRate belongs on the Y-axis for statistical validity\n\n\n\nThis plot shows that in nearly all departments, women had similar or higher admission rates than men. The illusion of bias in the aggregate comes from differences in application patterns, not unfair decisions within departments. To confirm this, let’s also show how department choice varied by gender.\n\napplicants &lt;- ucb_df %&gt;%\n  group_by(Dept, Gender) %&gt;%\n  summarise(Applicants = sum(Freq))\n\n`summarise()` has grouped output by 'Dept'. You can override using the\n`.groups` argument.\n\nggplot(applicants, aes(x = Dept, \n                       y = Applicants, \n                       fill = Gender)) +\n  geom_col(position = \"dodge\") +\n  labs(title = \"...\",\n       y = \"...\", \n       x = \"Department\") +\n  scale_fill_manual(values = c(\"Male\" = \"#377eb8\", \n                               \"Female\" = \"#e41a1c\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis second chart reveals that men were more likely to apply to departments A and B, which had higher acceptance rates, while women applied more often to departments C through F, where competition was steeper.\n\n\nWhat is the core difference between the two charts presented above?\n\n The first displays raw admission counts, while the second displays admission rates The first displays admission rates, while the second displays raw admission counts the first chart is inappropriate and brings no added value to the analysis, while the second chart is highly valuable the second chart is inappropriate and brings no added value to the analysis, while the first chart is highly valuable there is no core difference between the two charts; this is a trick question\n\n\n\n\n\nWrapping Up\nThis example is more than a historical curiosity—it’s a powerful reminder of what can go wrong when we skip visual exploration. Without breaking down the data into meaningful subgroups or visualizing it, we risk drawing misleading conclusions from aggregated numbers—a mistake that can easily obscure important patterns or biases hidden within the data. Here are a few key takeaways:\n\nNever Rely on Aggregates Alone Always ask: What groups might I be collapsing? Can different subgroups tell different stories?\nUse the Right Visualization for the Question Tables are great for precision, but bar plots, dot plots, and faceted graphics help reveal structure. In this case, side-by-side bar charts made the paradox visible in seconds.\nVisuals Help You Understand Your Own Data Good graphics aren’t just for presentations. They’re how you, as a researcher or analyst, come to understand the texture of the data you’re working with.\nTabulation Has Added Value When It Reveals Structure A well-designed cross-tab or grouped summary tells you what’s driving a result. Don’t just count things—count them strategically.\n\n\n\nHungry For More?\nFor more information on the exact data used in this post, check out the full paper here. Alternatively, type help(UCBAdmissions) into the R console if you wish to replicate or extend the analyses conducted here."
  },
  {
    "objectID": "posts/welcome.html",
    "href": "posts/welcome.html",
    "title": "Welcome to my website!",
    "section": "",
    "text": "Welcome to my website! My name is Conor O’Driscoll and I am an Assistant Professor in Economic Geography within the University of Groningen’s Faculty of Spatial Sciences.\nThis website will serve as a repository for research-related items, teaching materials, and other academic things people may find interesting. Any material borrowed from this website and used in the public domain should be referenced appropriately, but otherwise, I hope you enjoy it and find use in some of it. Below, I provide an outline of what you can expect from, and find on, this website."
  },
  {
    "objectID": "posts/welcome.html#miscellaneous",
    "href": "posts/welcome.html#miscellaneous",
    "title": "Welcome to my website!",
    "section": "Miscellaneous",
    "text": "Miscellaneous\nOther elements of this site contain a copy of my cv in pdf and raw html form as well as a News and Musings section in which you can find personal and professional updates, alongside blog posts related to my research and teaching activities.\nHappy reading and looking forward to seeing how this project evolves and grows! Conor"
  },
  {
    "objectID": "posts/three_ps.html",
    "href": "posts/three_ps.html",
    "title": "The Three P’s: Programming, Packages, and Projects",
    "section": "",
    "text": "Starting To Programme in R\nOne of the strong points of R (and RStudio) is its extensive community. It is very easy to find answers to questions about functions or commands on Google (using sources like Stack Overflow, Cross Validated, Reddit, and R Bloggers) or ChatGPT. So, whenever you have a problem, a question or a doubt, try these tools to find the answer!\nMoreover, R contains a help repository about all the functions. You can use the help() function to access the supporting material related to a specific function or package. You can also find a help tab in the bottom-right corner of RStudio. Another very useful resource is represented by the R Cheatsheets. Those are compacted documents with some of the most useful functions to work in R in different topics and applications: https://posit.co/resources/cheatsheets/?type=posit-cheatsheets&_page=1/\nOne problem shared by these resources in general is that they require you to know what you are talking about when describing R-related queries and problems. From the community forum angle, this is important because this community is so diverse in its expertise and background that oftentimes, different levels of R knowledge are assumed, and different R conventions are employed. Other times however, it is simply because you can describe the same problem in one hundred different ways, but some ways are better, and more efficient, than others. Another problem not really faced by the forums but very much relevant to OpenAI is the varying quality of solutions. Answers in forums are judged by the community while answers in OpenAI are judged by you, the person who has the problem to begin with.\nTo competently navigate this landscape therefore, you need to (roughly) know what you are looking for, and in this case that basically means learning how to “read code”. To read code, you must either gain extensive experience in working with code (i.e., through research), or you must study coding in some capacity. It is worth pointing out that I believe that the best way to learn how to do statistical analyses and to code is to practice. Studying these resources will only get you so far, and in our experience, doing statistics and using R in real life is the best way to develop proficiency. In this way, learning statistics and R is quite literally like learning a new language.\n\n\nNavigating Errors: Functions and Packages\nR requires a high level of precision when working with it. For example, it is case sensitive. If you have a dataset called Dataset and you tell R to create some graphs using dataset, your code will not work. Similarly, some functions and commands only work with a specific type of data/variable. Finally, if you tell R to import a dataset or a file, and that file is not stored in the correct directory (i.e., the project directory/folder), R will not be able to find it, and your code will not work.\nR will generally tell you what is wrong with some degree of clarity, but sometimes these error messages can be cryptic. Most of the error messages you are likely to encounter early on in your R experience are, fortunately, relatively straightforward to fix. Many of you are likely to encounter errors before you do any data work.\nSome errors will occur if there has been problems loading in the dataset you wish to use. The contents of this message will usually read Error in [code]: object [code] not found or Error in [code]: object [code] not found. In nearly all cases this is the result of either misspelling the dataset name or it is the result of R not being able to find the dataset you are referring to (more on this below). Remember you must load the dataset into R every time you want to use it!\nAnother error occurs when you basically make a typo in your code. For example, you spell a variable name wrong, or you put a comma in the wrong place. In these cases, you may get errors which say something like Error: Unexpected […] in [code]. This message means that, whatever code you have just tried to run, R is having difficulties making sense of it. More specifically, R thinks there is a mistake and will not be able to execute the code until you make changes. In many cases, R may even tell you precisely where this code is, often in the form of “lines and chunks”. For example, it might say this Error occurred in Chunk 1 and line 20 – providing you with clear instructions on where to look for this mistake.\nR comes with some basic built-in functionalities. For example, you can use R to do simple arithmetic operations like addition, subtraction, multiplication, and division. You can also use R to create variables, store data, and manipulate data in various ways. However, the power of R really comes from the fact that you can add extra tools to it through packages. These packages contain pre-written code that you can use to do more advanced tasks that may not be possible or efficient with just the basic functionalities of R.\nEvery function in R requires a certain number of inputs (so-called “arguments”) so that the function can work properly. For example, you cannot calculate the mean of a variable without first specifying data points. Or you cannot draw a bar chart without first picking the relevant variables. Your function will not work if you specify your inputs/arguments incorrectly. If you make a mistake of this nature, the error may read as Error in [function]: argument [argument] is missing, with no default or Error in [Function(Argument)]: unused argument [argument]. To solve these errors, you need to make sure that function is being used properly (read the help documentation!) and that all inputs are correctly specified.\nR packages extend the default functionality of R by providing additional functions, data, and documentation. They are written by the worldwide community of R users. You can think of R packages like apps that you can install on your mobile phone. In order to access the functions and the data contained in one specific R package, you need to install the package (you only need to do this once, if the package has not been installed before, exactly like an app) and you can do this using the install.packages() function.\nOnce an R package has been installed, you need to open it (you need to do this in every session in which you use that R package) and you can do this using the library() function. This is necessary because there are so many packages in R that many share the same functionality and contain the same commands.\n\n\n\n\n\n\nWarning\n\n\n\nA consequence of this is that some packages do the same things and employ the same name for commands (i.e., filter()) even though these commands are slightly different in how they work in different packages. This creates what are known as “conflicts” (i.e., the same command name is used for two distinct tasks). By telling R that you specifically want to use, say, the tidyverse package, you are reducing the chance that conflicts will emerge. If that does not work however, R will clearly signal this and tell you which commands get priority. In general, commands from packages “activated” last get priority. Conflicts are not a problem in themselves, but understanding what they are and why the occur will prove extremely useful as you come to grips with R.\nFor some additional information on how to install and load packages in R by watching the following video: https://www.youtube.com/watch?v=- wTcDqJWSzA&list=PLGPGLQRuosDKwjTbOknj12ASPFpJ-V6vK&index=8 \n\n\nAnother common error message you are likely to encounter will read something like Error in [code]: Could not find function [code]. This error means you are trying to use a function belonging to a package which is not yet installed or loaded. To solve this error, you have to install the package (if it is not installed yet) and load it with the install.packages() and library() functions. Remember that every time you start R, you must load in any packages you want to use!\n\n\nR Projects\nAs your work in R grows, it becomes important to organize your files and maintain reproducibility. This is where R Projects come in. R projects are organized workflows within the R programming environment, designed to manage code, data, and outputs in a structured way.\nFor those of you with experience using other statistical software, such as STATA, SPSS, or GIS, the concept of a “working directory” may sound familiar. A working directory is what R assumes to be your workspace, and when looking for files to import, or when exporting files, the first place it will go to is this directory. By default, RStudio will pick the user-directory on your computer or the directory “My Documents” as this workspace, so this is what you usually see in the Files tab the first time you open RStudio.\nProjects are basically R’s equivalent of a “(working) directory”. This is where R looks for files that you ask it to load, and where it will put any files that you ask it to save. Indeed, keeping all the files associated with a given project (input data, R scripts, analytical results, and figures) together in one directory is such a wise and common practice that RStudio has built-in support for this via projects.\nEach project has its own R session, helping to prevent objects from other projects from interfering. For more advanced users, R projects integrate seamlessly with Git, allowing you to track changes over time. This is actually how I keep this website and these blog posts up-to-date.\nTo create a new project, you can navigate to the top-right corner of RStudio (below the red X and above the global environment). From there, you can either open an existing project, or create a new one. For creating a new project, you will normally go through the following steps:\n\nClick File &gt; New Project OR Click Project: (None) &gt; New Project \nIf you have not made a dedicated folder already for your project, click New Directory. If you have made a dedicated folder already, click Existing Directory. More advanced users to integrate Version Control should they wish to do so, but I will not cover that here. \nIf you clicked New Directory in 2., you need to decide what type of project you will make. The most straightforward is the first option (New Project), but you can also click Quarto Project if you intend to use quarto files as the main source of analysis. \nIf you clicked Existing Directory in 2., you need to tell R where the folder you want your R project to live in is located and what it is called. You need to do this so R knows where to save all the relevant project files. \nFollowing from 3a, you need to give your directory a name. This will become the name of the folder which stores your R project. The box below Directory Name allows you to tell R where you want this folder to be stored. \n\nProjects provide a dedicated folder structure for storing all files related to specific pieces of analysis. R projects help keep files organized, but also make it easier to collaborate and share your work. They do this by providing users with consistent links and file paths, and therefore simplify reproducibility and portability across different systems. Thus, when you create a Project File, the working directory for that project will automatically be set to wherever you saved this file. Once created, you should see that instead of saying Project: (None), that icon on the top-right corner now displays the project name.\nEach project usually contains a certain number of files and has a particular structure. Part of this structure can be intuited through the cover picture of this post, but some more specific details on the files you need to keep together are provided below:\n\nAn R Project File (.Rproj) \nA Rproj.user folder \nA Rhistory source file \nSome file for storing your code/analysis (e.g., an R Script (.R) or a Quarto file (.qmd)) \nAn R Workspace \n\nThe folder that contains 1), 2), and 3) is the ecosystem of your analysis; this is your project directory. Anything outside of this folder will not be included by default, nor will you be able to work with it in R – any file you want to include in your analysis must be in this project directory. 4) is the file you will use to conduct your analysis. That is, the file where you can insert code, generate tables and graphs, and produce statistical outputs. 5) is the dataset you will use throughout the research.\nIf you are a student in my University of Groningen courses, you do not need to create R Projects from scratch. Nor will you have to create Quarto documents and independently source data sets. Rather, we do all that work for you. This means that you will have everything you need to get started on your assignments right away, meaning you can open the Quarto document right away and begin working. Once you leave this course however, things won’t be so easy, so it is important that you learn how these projects and files work. The best way to do this is to practice by making your own and experimenting with them, but here are some useful resources you can use to learn more about them:\n\nWorking with Scripts and Projects: https://r4ds.hadley.nz/workflow-scripts.html\nUsing R Projects: https://support.posit.co/hc/en-us/articles/200526207-Using-RStudio-Projects\nUsing R Projects: https://www.youtube.com/watch?v=MdTtTN8PUqU"
  },
  {
    "objectID": "posts/thinking.html",
    "href": "posts/thinking.html",
    "title": "Thinking Statistically: A Primer",
    "section": "",
    "text": "Thinking Statistically: A Way of Seeing the World\nMost of us meet statistics in passing — a poll on the evening news, a headline about a medical breakthrough, a sports commentator flashing charts on the screen. We may not think much of it, but statistics quietly runs through almost every part of modern life: from predicting the weather, to setting housing policy, to planning bus timetables. Behind every one of those applications is the same motivation: to make better sense of the world around us, thus helping us to make better decisions in the face of uncertainty.\nFor social and spatial scientists, this work happens in two worlds at once. The visible world (e.g., house prices, commuting patterns, school applicatins) helps us to come up with our own ideas and theories about how the world works. Meanwhile, the world we infer (e.g., migration patterns, economic opportunity, discrimination, and efficiency) is the world we inhabit when we try to link our theories and ideas to reality.\nStatistics sits at the intersection of these two worlds. Without statistics, our ideas and theories about the world around us are little more than speculations. With statistics, we can compare our perceptions of the world to a more grounded view of reality, allowing us to test and validate our own understanding. A poor match between our ideas and reality forces us to think again and reformulate our theories based on what we observe through statistics, while a good match might signal that we have a good intuitive understanding of what is going on.\nImplicit in this is the notion that, in order to be meaningful, our ideas and theories of the world around us must yield plausible predictions; predictions which we can evaluate and assess using statistics. If our theories and ideas to do not tell us what we should expect to observe, or if the predictions are so general that any data will conform with our theories (e.g., astrology), then evidently our theories and ideas are no good.\n\n\nDecision Making and Asking Better Questions\nWith this in mind, statistics is also a valuable tool we can use to steer our way through a complex world and make decisions about the “best” actions to take. We can use statistical methods to extract information from data we have collected to describe how the world is behaving and infer how we might wish to respond. Therefore, statistics plays a fundamental role in tying observations about the world around us to our ideas and understanding of that world, providing a disciplined way to make sense of a messy reality.\nThe kind of thinking involved in Statistics may seem quite different from what you are used to. But once we strip away the jargon and examine what it is we are really doing, you will quickly realize that the kind of thinking involved in statistics is not entirely new to you. In fact, many of your day-to-day assumptions and decisions already depend on statistical thinking.\nWhen we summarize our past experience in a rough-and-ready way, or generalize from previous experiences, and use this information to make predictions about what we expect, we are subconsciously making broader sense of our lived experiences. That is why thinking statistically is not about chasing certainty, it is about navigating ambiguity with discipline. We use data to challenge our assumptions, to explore patterns we might not otherwise see, and to resist being misled by what seems obvious.\n\n\n\n\n\n\nNote\n\n\n\nSuppose I tell you that I have two friends: one is around six feet tall and the other is five feet tall. What would be your best guess as to each one’s sex?\nOf course, you have not seen all men, or all women, but experience probably tells you that by-and-large, men tend to be taller than women.\nTherefore, I expect that you feel fairly confident in assuming that my smaller friend is female, while my taller friend is male. In other words, in the absence of any other information, you probably think it is more likely that a tall adult is male and a small adult is female.\n\n\nWe do this all the time. When you say: “On average, I cycle 50km per week”, or “We can expect a lot of rain at this time of year”, or “It is more likely that you will do well in your exams if you begin studying early” you are making statistical statements even though you have performed no calculations. Indeed, each of these examples is showing off one particular type of statistical thinking; answer the questions below to figure out which.\n\nWhat type of statistical thinking best corresponds to each of the following scenarios:\n\nOn average, I cycle 50km per week. GeneralizingPredictingSummarizingAssessing\nWe can expect a lot of rain at this time of year. GeneralizingPredictingAssessingSummarizing\nIt is more likely that you will do well in your exams if you begin studying early. AssessingPredictingGeneralizingSummarizing\n\n\n\n\nStatistics is human-made\nBecause statistics involve numbers, people often assume they are hard facts handed down by nature. But numbers don’t just fall from the sky — people choose what to count, how to count it, and how to present it. That means statistics are not just facts, they are interpretations.\nTo think statistically is to think critically. It’s to ask questions like: Is that plausible? Compared to what? How do they know? The more you practice, the more these habits become second nature — a kind of internal, healthy skepticism paired with external curiosity.\nThis ability to think critically is become more important everyday. We live in an age where information is instant but trust is fragile. It is becoming increasingly difficult to tell what is true and what is not. Even though misinformation has been a problem that has existed since ancient Greece, the unique problem we face today is that it has proliferated and is closely intertwined on the internet with real information, making it sometimes difficult to identify.\nSome of you who might respond by saying “it is not my job to evaluate statistics critically”. I say that it may not be your job, but it is your responsibility to own your opinions. Taking ownership of these opinions requires you to think critically about the words and numbers you encounter, and scrutinize/examine them as best you can.\nTest your ability to critically evaluate statistical statements using the following examples, where TRUE = Plausible and False = Not Plausible:\n\n\nA crystal wine glass fell off a table onto a thick carpet without breaking. TRUEFALSE\nA crystal wine glass fell off the roof of a forty-storey skyscraper onto the footpath without breaking. TRUEFALSE\nThe best salesperson in Company X makes 1,000 sales every day. TRUEFALSE\nThe cost of a telephone has decreased by 12,000 percent since the formation of the Communication Satellite Corportation. TRUEFALSE\n\n\nProblems 4 and 5 were hopefully easy for most of you.\nProblem 6 is a bit trickier because it is not immediately obvious where to begin. But below I take a stab at it. Not to say that this is the one-and-only way to answer this question, but it is one illustration of how you can apply statistical thinking to assess the plausibility of a statistical statement.\n\n\n\n\n\n\nNote\n\n\n\nAssuming that it takes: i) five seconds to dial a phone number, ii) another five seconds for the phone to ring, iii) 10 seconds to deliver your sales pitch, and iv) 40 seconds to collect the buyers bank details and address. That adds up to one call per minute. Finally, lets assume that every phone call ends in a sale. Even under these highly optimistic conditions, you come out sixty sales per hour, and 480 sales in an eight hour work day with no breaks. Need I say any more?\n\n\nClaim 7 is difficult to verify if you do not spot the immediate contradiction. If a cost decreases by 100 percent it means that, irrespective of the starting price, the price is now zero. If a cost decreases by 200 percent, it means that someone is paying you the same amount of money that you used to pay them for their product. Thus, while a decrease in price of 100 percent is quite rare, a decrease of 12000 percent seems wildly unlikely. You might think this is obvious, but tell that to the editors of Science (a premier scientific journal) who published such a statement.\n\n\n\n\n\n\nNote\n\n\n\nClaim 7 introduces another common error when interpreting statistics and that involves making sense of percentages.\nPercentages seem so simple and incorruptable, but they are often confusing. For example, if your salary drops by 50% tomorrow, you will not “break-even” by increasing your new salary by 50 percent because the baselines have changed.\nIf you were recieving €1,000 each week and that suddenly dropped to €500 (a 50 percent reduction), a 50 percent increase on that pay only brings you to €750.\nSimilarly, if interest rates rise from 3 to 4 percent, that constitutes an increase of one percentage point, or an increase of 33 percent. Meanwhile, if interest rates drop from 4 to 3 percent, that constitutes a decrease of one percentage point, but a decrease of 25 percent. Has your head exploded yet?\n\n\nWorking with percentages and probabilities can be hard even for those of us with advanced degrees in such subjects. So even though we all practice statistical thinking quite regularly, that does not mean we are naturally very good at it.\nIn one somewhat famous case, the state of New Jersey (U.S.A) adopted legislation that denied additional benefits to mothers who have children while already on welfare. Some legislators believed that women were having babies in New Jersey simply to increase the amount on their welfare checks. Within two months, legislators were declaring that the “family cap” law was a success because births had already fallen by 16 percent, as reported in the New York Times. Now, maybe things are different in New Jersey, but I believe that it takes nine months for a pregnancy to come to term, so how can this effect possibly be attributed to the policy?\nBut state legislators are not the only people to make basic blunders in statistical thinking. Experts do it all the time, something demonstrated in the highly impressive Thinking, Fast and Slow by Daniel Kahneman (Nobel Laureate in Economics, 2002).\nOther claims, like More people have cell phones than toilets (reported by Time Magazine in 2013) are a bit trickier because the answer depends on a host of factors, like who/how we count and what we compare. One potentially reasonable place to start in this example might be the observation that many people in the developing world do not have indoor plumbing, whereas many people in the developed world may have more than one cell phone (at least over their lifetime). From this angle, the claim seems plausible. This is not to say that we should accept it, but rather that we cannot reject it out-of-hand as being ridiculous. In this case, what matters isn’t whether you instantly know the answer, but whether you know how to start thinking about solving the problem.\n\n\nShooting the messenger\nImportantly, however, not all misuses of statistics are accidents. In his book A Field Guide To Lies and Statistics, Daniel Levitin (a neuroscientist) provides a number of examples where statistics are mis-used, misinterpreted, or misapplied honestly as well as dishonestly.\nThis hits upon a broader point that sometimes you cannot always determine whether a statistic or claim is reliable. Sure, the people who generate and report on such statistics should do this for you, but they often do not. Sometimes, this is deliberate (i.e., politically-motivated) but other times the people generating/reporting on these statistics do not understand them all that well.\nThat is how runaway statistics take hold, potentially poisoning public opinion and discourse on a particular topic, further emphasizing the importance of being able to think statistically.\nTake the widely reported statistic (from some years ago) that, in the U.S. 150,000 girls and young women die of annorexia each year. According to the U.S. Centers for Disease Control, the annual number of deaths from all causes for girls and women between the ages of fifteen and twenty-four (a reasonable definition for “girls and young women”?) is around 8,500. If you become more conservative in your definitions and incorporate women aged twnety-five to fourty-four, you still only get 55,000. It should be obvious that annorexia deaths in one year cannot be three times the number of all reported deaths, but it clearly was not.\nIn these cases, it is often very easy to blame to blame the statistician or the statistics themselves, but they are usually not the ones with concealed motivations, political or otherwise.\nMoreover, when a statistic is used to measure performance, people often start “playing to the metric” — sometimes in ways that make the number look better without actually improving the thing it’s meant to measure. (Economists call this Goodhart’s Law). National accounting measures like Gross Domestic Product (GDP) are becoming increasingly good examples of i) statistical measures that are gamed for concealed motives and ii) measures which do not do what they say on the tin - which brings us to a whole new level of discussion about picking and understanding your choice of measurement…the subject of another blog.\n\n\nWrapping Up\nStatistics is not just about techniques. It is about developing better instincts and organising your curiosity. It helps you ask clearer questions and give more careful answers.\nIt will not turn you into a calculator. But it will help you see more clearly. To doubt usefully, not cynically. To ask things like: Where did that number come from? What is the comparison? What is the alternative?\nIf you have ever had a hunch something was not quite right, or spotted a pattern in your surroundings, congratulations. You have already started thinking statistically.\n\n\nBibliography\n\nStatistics Without Tears: An Introduction For Non-Mathematicians, by Derek Rowntree\nA Field Guide To Lies and Statistics: A Neuroscientist on How to Make Sense of A Complex World, by Daniel J. Levitin\nElementary Statistics For Geographers by James Burt, Gerald Barber, and David Rigby"
  },
  {
    "objectID": "posts/samp_pop.html",
    "href": "posts/samp_pop.html",
    "title": "Populations and Samples",
    "section": "",
    "text": "When we use statistics, we are always trying to say something about the world. But the world is big, messy, and full of variation. So the first step in any statistical study is to decide what part of the world we are trying to understand. This is where the idea of a population comes in — and why, in practice, we usually rely on samples instead.\n\nStatistical Populations: The Ideal Starting Point\nIn statistics, a population is the complete set of individuals, objects, or events that share a common characteristic of interest - a characteristic that we wish to study.\nIf we are studying election outcomes, the population might be all eligible voters in a country. \nIf we want to know the average height of trees in a national park, the population is every tree in that park. \nIf we are testing the reliability of a product, like lightbulbs, the population could be all lightbulbs manufactured by a company. \nIn each case, the population is the entire universe of cases we want to understand. This makes them conceptually appealing because they capture everything. If we could measure an entire population, we would have no uncertainty — we would know the true average tree height, the exact proportion of voters supporting each candidate, or the precise distribution of lightbulb lifespans. This is the logic behind a census, where every single member of a population is measured. Censuses are the statistical ideal, because they eliminate the guesswork.\nThe most well-known example of a census is, of course, a national population census, which attempts to count every single person in a country. These efforts are monumental undertakings, requiring years of planning, vast budgets, and enormous data-collection infrastructures - highlighting some of the difficulties associated with reaching this ideal.\nUnfortunately, studying populations directly is rarely feasible. National population censuses are the exception, but even they are massive undertakings requiring years of preparation, vast budgets, and armies of workers. Outside of such special cases, researchers usually cannot access an entire population. There are several reasons for this:\n\nCost and time: Measuring every single unit is almost always too expensive and too slow.\nLogistics: Some populations are too large or widely dispersed. Measuring every tree in the Amazon rainforest is simply impossible.\nDestructive measurement: Sometimes, testing requires destroying the unit. You cannot measure the lifespan of every lightbulb if each test means running it until it burns out.\nChanging populations: Populations are often dynamic. People are born, products roll off production lines, ecosystems evolve. By the time you measure everyone, the population itself has already changed.\n\n\n\n\n\n\n\nNote\n\n\n\nIt is also worth noting that defining the population itself is not always straightforward. Sometimes it is clear and obvious (e.g., all registered voters in an election). Other times, it requires careful thought. What counts as the relevant population when testing a new medicine? Just the trial participants, or everyone who might one day take the drug? Defining the population is often a conceptual decision, not just a logistical one. Similarly:\nA survey of 2,000 voters is a sample from the population of all voters. If the sample includes people of different ages, regions, and political leanings in roughly the right proportions, it can give us reliable insights into voting intentions.\nMeasuring 50 trees from different areas of a park is a sample from the population of all trees. If the trees are chosen carefully from across the park, the sample average height will be a good approximation of the population average.\nTesting 100 lightbulbs is a sample from the population of all bulbs made by a manufacturer. If those bulbs are chosen randomly, their measured lifespan can be generalized to the entire production line.\n\n\nThis is why most statistical studies must settle for something more practical: sampling.\n\n\nSampling: Building a Bridge to the Population\nBecause populations are so hard to study directly, we work with samples. A sample is a subset of the population that we actually collect data from. It is the part of the population that we observe directly, with the goal of using it to make claims about the whole.\nBut a sample is not just “a smaller group.” The power of a sample comes from the fact that each member of it shares some common attribute that defines the population we are interested in. Every tree in our sample is still a tree in the park. Every voter surveyed is still an eligible voter. Every lightbulb tested still came from the manufacturer’s production line. This shared attribute makes it meaningful to ask: how do characteristics (like height, opinion, or lifespan) vary across different members?\nAt the same time, no two members are identical. Each tree has a slightly different height. Each voter has their own political leanings. Each bulb lasts a different number of hours. Some of these differences matter more than others — and the whole point of sampling is to understand and measure these variations in a way that allows us to make statements about the broader population.\nConsidering this, the value of a sample lies in its representativeness. Sampling allows us to use a manageable amount of data to make inferences about the larger population. The underlying idea is that, if the sample is chosen carefully, the information it provides will reflect the characteristics of the whole. That is, a good sample reflects the key characteristics of the population, so that any patterns we observe in the sample mirror those in the larger group.\nHowever, not all samples are equally useful. A good sample has two key qualities:\n\nRepresentativeness: The sample should reflect the important characteristics of the population. If young people and older people have different voting patterns, a representative sample must include both groups in appropriate proportions.\nSufficient size: Larger samples tend to provide more precise estimates, because they reduce the role of chance. That said, size alone is not enough — a large but biased sample can still lead to misleading conclusions.\n\nImagine stirring a pot of soup. You don’t need to drink the entire pot to know how it tastes — you just need a spoonful. But the spoonful must be taken after stirring, so that it is representative. If you scoop only from the top, you might end up with just broth, missing the vegetables and seasoning below. Sampling in statistics works the same way: the sampling method matters as much as the size.\nSampling method is about how you select the units, and we will discuss this in more detail in another post. Whereas sample size affects how much variability we expect by chance. With small samples, results can swing widely just due to randomness. With larger samples, estimates stabilize and become more reliable. That said, bigger is not always better. A small, well-designed random sample can be more reliable than a large, biased sample. For example, a carefully randomized survey of 1,500 people can be more accurate than a self-selected online poll of 100,000.\nThe ideal is a sample that is both representative and large enough to provide stable estimates.\nEnsuring that a sample is truly representative of a wider population requires a surprising amount of work. Bias can creep in at almost any stage of the process: from how the population is defined, to how the sample is selected, to who actually responds. For instance, even if you start with a random sample, you may run into nonresponse bias if certain groups are less likely to answer your survey than others, making response rates particularly important for survey data. Similarly, coverage bias can occur if your sampling frame leaves people out altogether, such as surveying only landline users when many younger people rely exclusively on mobile phones. Even small design choices — like where data is collected, when surveys are distributed, or how questions are worded — can systematically shape the kinds of people who participate and the answers they provide.\n\nTry your hand at some of these questions to see whether you truly get the logic of sampling. For each scenario, identify whether the chosen sample is truly representative of the wider population.\nThat is, given the information provided, select TRUE in cases where you believe the samples are representative and that there is little-to-no risk of systematic bias entering the estimates, and FALSE when these conditions do not hold.\n\nA study wants to understand the travel behaviours of university students across the Netherlands. Researchers collect data only from students living and studying in Groningen. TRUEFALSE\nA company wants to understand job satisfaction across its entire workforce. Researchers select 300 employees, making sure to include exactly 150 men and 150 women. TRUEFALSE\nA pre-election survey randomly selects 2,000 names from the national register of voters at random and asks about voting intentions. All 2,000 individuals respond truthfully. TRUEFALSE\nA study wants to understand attitudes toward climate change in the United States. Individuals are randomly selected from San Francisco to complete a survey on the topic. The survey had a 100% response rate. TRUEFALSE\nCity planners in Utrecht want to estimate average household size in the city. They randomly select several neighborhoods across the city and survey all households in those areas. The surveys have a 100% response rate. TRUEFALSE\n\n\nBelow are some more exercises to really drive home the point that representativeness is quite hard to achieve in practice and requires a lot of work. Clearly, some of the examples (i.e., the distinguishing factors between 10 and 11) are artificial and unrealistic in many ways. But they do serve as a useful thought exercise for thinking about bias, the topic of our next section.\n\nTry your hand at some of these questions to see whether you truly get the logic of sampling. For each scenario, identify whether the chosen sample is truly representative of the wider population.\nThat is, given the information provided, select TRUE in cases where you believe the samples are representative and that there is little-to-no risk of systematic bias entering the estimates, and FALSE when these conditions do not hold.\n\nResearchers in the RUG want to explore citizen satisfaction with amenities in the City of Groningen. To achieve this, they conduct street interviews at distinct spots in the city on one Saturday afternoon in October. TRUEFALSE\nResearchers in the RUG want to explore citizen satisfaction with amenities in the City of Groningen. To achieve this, they conduct street interviews at distinct spots in city on twelve random Saturday afternoons throughout the year, ensuring to capture responses in every month. TRUEFALSE\nResearchers in the RUG want to explore citizen satisfaction with amenities in the City of Groningen. To achieve this, they conduct street interviews across the city on twelve random Saturday afternoons throughout the year, ensuring to capture responses in every month. Prior to each Saturday, the interviewees randomise the locations they will visit. TRUEFALSE\nResearchers in the RUG want to explore citizen satisfaction with amenities in the City of Groningen. To achieve this, they conduct street interviews across the city on twelve random Saturday afternoons throughout the year, ensuring to capture responses in every month. Prior to each Saturday, the interviewees randomise the locations they will visit. Every 5th person to walk by the interviewer is asked to interview. Not everyone that is asked agrees. TRUEFALSE\nResearchers in the RUG want to explore citizen satisfaction with amenities in the City of Groningen. To achieve this, they conduct street interviews across the city on twelve random Saturday afternoons throughout the year, ensuring to capture responses in every month. Prior to each Saturday, the interviewees randomise the locations they will visit. Every 5th person to walk by the interviewer is asked to interview. Everyone that is asked agrees, but you are not convinced that everyone answered truthfully. TRUEFALSE\nResearchers in the RUG want to explore citizen satisfaction with amenities in the City of Groningen. To achieve this, they conduct street interviews across the city on twelve random Saturday afternoons throughout the year, ensuring to capture responses in every month. Prior to each Saturday, the interviewees randomise the locations they will visit. Every 5th person to walk by the interviewer is asked to interview. Everyone that is asked agrees, you know that everyone answered truthfully. TRUEFALSE\n\n\n\n\nBias and Generalisation\nA major threat to useful sampling is bias. Bias occurs when the sample systematically differs from the population in ways that matter for the outcome of interest.\nA representative sample captures the diversity of the population, while a biased sample systematically excludes some groups or overrepresents others. If we conduct an online poll about political views, but only younger people respond, the sample may not represent the full voting population. If we only measure trees close to the park entrance, we might miss variation in tree heights deeper inside the forest. If we test only lightbulbs from the beginning of a production run, our estimate of lifespan may not reflect bulbs produced later. Bias can creep in through poor sampling design or low response rates. Recognizing and minimizing these risks is a central concern in applied statistics.\nFor example:\nIf a voter survey only includes people with internet access, it may miss older or rural populations. \nIf tree measurements are taken only near trails, the sample may not reflect conditions deeper in the forest. \nIf product tests only use the first items produced in a manufacturing run, they may not capture variability in later batches. \nBias undermines the goal of sampling: to generalize to the population.\nOne of the strongest tools to mitigate bias is randomization. Randomly selecting units ensures that, on average, the sample will resemble the population. Randomization does not eliminate all differences — chance variation remains — but it removes systematic distortions. This is why randomization is central not only to survey design but also to experiments, where random assignment of participants helps balance out hidden differences between groups. The guiding principle is always the same: the purpose of sampling is to learn about the population, not just about the sample itself.\nTry your hand at some of the following questions. In situations where you think the results of these studies are generalised appropriately, click TRUE. If the generalisations are not appropriate, click FALSE.\n\n\nA survey of 100 residents randomly distributed across the Groningen municipality is used to conclude that all municipalities in the Netherlands should adopt the same traffic-calming measures. Note: this sample is representative of the population of Groningen province. TRUEFALSE\nA survey of 500 students at University College Cork is used to conclude that all university students in the Ireland prefer online lectures to in-person lectures. Note: every faculty and department in University College Cork is represented and the sample as a whole is representative of the student body in University College Cork. TRUEFALSE\nA representative sample of 500 students studying in 5 of the 8 universities in Ireland is used to conclude that all university students in the Ireland prefer online lectures to in-person lectures. TRUEFALSE\nA nationally representative survey of 2,500 adults, randomly sampled across regions, ages, ethnicities, and genders, is used to conclude that approximately 60% of adults in the country report having visited a museum in the past year. TRUEFALSE\nA survey of 1,500 households living in urban areas is used to conclude that rural areas require greater investment in public transport infrastructure. TRUEFALSE\n\n\nWhen drawing conclusions from a sample, it is important to remember that not all samples support broad generalizations. Even if a sample seems well-constructed, its scope, size, or coverage may limit the extent to which findings can be applied to the wider population. Overgeneralizing from small, localized, or biased samples can lead to misleading conclusions, particularly in contexts like regional policy, where local conditions and preferences vary widely. Always consider whether the sample truly reflects the diversity and characteristics of the population before making sweeping statements. When in doubt, it is safer to frame findings as observations about the sample itself rather than the population as a whole.\nThe ultimate reason we sample is to generalize. By carefully selecting and analyzing a subset of the population, we aim to draw conclusions about the whole. If a sample is representative and unbiased, the patterns we observe — average values, relationships between variables, differences between groups — can be taken as evidence of what exists in the population.\nThis is the engine of statistical inference: starting with the limited, and carefully reasoning our way to the broader."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Conor O’Driscoll",
    "section": "",
    "text": "twitter\n  \n  \n    \n     linkedin\n  \n  \n    \n     google scholar\n  \n  \n    \n     university webpage\n  \n  \n    \n     github\n  \n\n  \n  \nI am an Assistant Professor in Economic Geography at the University of Groningen’s Faculty of Spatial Sciences. I recieved my PhD (Economics) from University College Cork, Ireland, where I was based in the Spatial and Regional Economic Research Centre (SRERC) between 2020-2023.\nI study how location shapes economic and social outcomes for individuals, focusing on the factors driving location decisions and the dynamic relationship between geography, mobility, and economic opportunity. I do quantitative research in Economic Geography, Regional Science, and Transportation Studies. In 2023, I won the Postgraduate Researcher of The Year (2021-2022) at the University College Cork College of Business and Law.\nI teach courses in Introductory and Intermediate Statistics, Quantitative Research Methods, and Economic Geography. I supervise Bachelor Theses in our Human Geography and Planning programme, as well as Master Theses in our Economic Geography and Real Estate Studies programmes. I am the Chief Organiser of the Research Clinic series hosted by the Department of Economic Geography and I serve on the Faculty Council in the Faculty of Spatial Sciences\nIn my free time, I like to run, play chess, cook, and read. I am a big Track and Field fan, but also enjoy watching football (soccer). I have had the honour of representing the Republic of Ireland 4 times in competitive athletics and have won numerous national titles across four track events.\n\n\n\nThanks for checking out my web site! If you have questions or wish to collaborate, you can reach me at c [dot] odriscoll [at] rug [dot] nl"
  },
  {
    "objectID": "Data.html",
    "href": "Data.html",
    "title": "Data, Data, Data",
    "section": "",
    "text": "Introduction:\nWhat Is Data?\n\ndefinition;\ndescription;\ndatasets [pp.16-24 in textbook] - link to from observation to data\n\nCollecting Data\n\npp.16-24 in textbook\n\nData: Some Key Considerations\n\nWhat are you measuring?\nHow are you measuring it?\nHow meaningful are your measures?\n\nIntroduction\nData is the core input of statistics. But more than that, they are the raw material on which the discipline of statistics is built, as well as the raw material from which individual statistics themselves are calculated.\n\n\n\n\n\n\nNote\n\n\n\nThe word data itself comes from the Latin datum, basically meaning “something given”. As such, many people opt to describe data as a plural term (i.e., the data are poor), rather than as a singular (i.e., the data is poor). But it is also common to treat the word data as if it were a continuum. For example, you would never say the water are wet. Rather, you would say water is wet. I tend to jump between both depending on the context.\n\n\nData are typically numbers: the results of measurements, counts, or other processes. But it is not always numbers. Sometimes, data represents categories, or it could even represent plain text, sentences and pictures, depending on the context in which you are working. In saying that, closer examination will usually reveal that when you try to use such data in any type of analysis, it is usually transformed in some way such that it becomes numeric.\nIt is all well-and-good saying that statistics primarily deals with numeric data, but for this data to be useful - that is, for us to be able to do some meaningful analysis - these numbers need to mean something. For example, we need to know, first-and-foremost, what the measurements we have collected are measurements of, and just what has been counted, when data is expressed in counts. Further, to produce reliable and accurate results, we also ought to know a fair deal about how these numbers were collected, alongside who and where they were collected from.\nDid everyone we asked give answers to our questionnaire, or did only some people answer? If only some people answered, are they properly representative of the wider group we wish to study, or has it been distorted in some way? For example, did a disproportionately large number of males answer our survey? Did people only answer specific questions? If so, how do we handle the missing data for the remaining questions? Is there something specific about that question that people did not like, or do these questions appear to have been avoided by specific groups of people (i.e., ethnic minorities).\nLikewise, we need to know if our data is up to date, and whether the measurement instrument we use is reliable. Can we assume that people accurately self-report their income, rent/mortgage payments, and commute times? Or are they better treated as a rough estiminate?\nThere are an infinite number of questions we could ask in this realm. What matters is that we stay alert to any which could influence the conclusions we draw. Failure to consider these questions is what vindicates people who are suspicious of statistics as they constitute a clear failure to apply and interpret the numbers appropriately.\nWith that in mind, another way of thinking about data is to consider it a form of evidence. Without data, our ideas and theories about the world around us are little more than speculations. Data provide a grounding, linking our ideas and theories to reality, while also allowing us to test and validate this understanding. Statistical methods can then be used to compare the data with our ideas and theories, to see how good a match they are. A poor match forces us to think again and reformulate them based on what we observe in the data, while a good match might signal that we have a good intuitive understanding of the problem at hand.\nImplicit in this is that, to be meaningful, our ideas and theories must yield plausible predictions; predictions which we can also compare with our data. If our theories and ideas to do not tell us what we should expect to observe, or if the predictions are so general that any data will conform with our theories (e.g., astrology), then evidently our theories and ideas are no good.\nData also allow us to steer our way through a complex world and make decisions about the best actions to take. We take our measurements, count our totals, and use statistical methods to extract information from these data to describe how the world is behaving and what we should do to make it behave how we want. These principles are illustrated when we seek to evaluate complex regional or social policies.\nData plays a fundamental role in tying observations about the world around us to our ideas and understanding of that world.\nFor example, a relatively basic category like “sex” can take two categories: male and female. Literally speaking, these are words, but when using them in any form of quantitative analysis, they are converted into numbers (usually, 1 and 0), where a value of 1 might represent Males and a value of 0 might represent females or vice versa.\nIn his book, Statistics: A Very Short Introduction, David Hand makes the nice point that even when working with more complicated categories, like entire sentences, these can be processed into word counts, or measures of similarity between words and phrases; and that this is the sort of representation used by search engines like Google.\nI prefer to think of data as an umbrella term capturing any inputs you use when conducting an analysis which contributes to the formation of an answer to your proposed (research) question. In saying that, although many forms of data do not start out as numeric, they will eventually be transformed into numbers, making it somewhat fair to generalise that data is a numeric object; especially seeing as most forms of statistics require numerical data as inputs.\nInsert MCQ - Which of the following might count as “data”, given the above description.\nIt is convenient to think of data as providing a simplified representation of whatever we are studying; be it people, firms, or places. Admittedly, the representation would not be perfect, because many underlying characteristics of people, firms, and places cannot be neatly captured by numeric measures, but they often tell us enough such that the core characteristics of interest can be identified. In saying that, it is important to bear in mind that data quality is extremely important. Like in life, if you have poor material to work with then the results you generate will also be poor. We can do amazing things with statistics, but we cannot perform miracles.\n\nFrom Observation To Data: A Refresher\nSometimes, we observe a single thing — a person, a place, an event — and note several of its features. More interestingly, we often observe many things that are similar in some respects but quite different in others. We notice patterns across people, places, or time. We end up with a collection of observations, or, in the language of statistics, data.\nFaced with such a collection, it is natural to start comparing. What is the same? What is different? What might explain those differences? This is where statistical thinking deepens. The overarching question becomes: what can we learn from this data?\nStatistics helps us summarize what we have found so we can be clear about the facts. But it also helps us go further: to compare, to explain, and to predict. It reminds us to be cautious. What appears true in one context may not hold in another, as I am sure you are all acutely aware. If we want to generalize more confidently, the logical starting point is to gather more experience - that is, collect more data. This speaks to the fact that the more contexts, situations, and experiences we observe, the more confident we can be in drawing conclusions and making suggestions. In other words, if we want to generalize more confidently, we need more data, which happens to be the logic underpinning one of the most powerful ideas in statistics: the Central Limit Theorem."
  }
]